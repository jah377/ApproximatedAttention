Thu 16 Jun 2022 04:34:29 AM CEST
r30n7.lisa.surfsara.nl
uid=55639(jharris) gid=55199(jharris) groups=55199(jharris),46457(lisa_uva_gpu),50488(ssh_forwarding)
/home/jharris/Desktop/approx_attention
/home/jharris/Desktop/approx_attention/venvs/GPU_venv/bin/python
/home/jharris/Desktop/approx_attention/venvs/GPU_venv/bin/python

Check if packages installed correctly
Torch: 1.11.0+cu113
PyG: 2.0.4


==================================================
dataset: arxiv
method: bayes
model: SIGNff_SHA
iterations: 100
run_trial: false
config: SIGNff_SHA.yaml
train_file: hps_SIGNff_DPA.py
project_name: sffSHA_arxiv
method: bayes
metric:
  goal: minimize
  name: val_loss
parameters:
  ATTN_HEADS:
    value: 1
  BATCH_NORMALIZATION:
    value: 1
  BATCH_SIZE:
    values:
    - 2048
    - 4096
    - 8192
    - 16384
  CLASSIFICATION_LAYERS:
    distribution: int_uniform
    max: 3
    min: 1
  CLASSIFICATION_UNITS:
    values:
    - 128
    - 256
    - 512
    - 1024
  DATASET:
    value: arxiv
  DPA_NORMALIZATION:
    values:
    - 0
    - 1
  EPOCHS:
    value: 300
  FEATURE_DROPOUT:
    values:
    - 0.0
    - 0.1
    - 0.2
    - 0.3
    - 0.4
    - 0.5
    - 0.6
    - 0.7
  HOPS:
    values:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
  INCEPTION_LAYERS:
    distribution: int_uniform
    max: 3
    min: 1
  INCEPTION_UNITS:
    values:
    - 128
    - 256
    - 512
    - 1024
  LEARNING_RATE:
    values:
    - 1.0
    - 0.1
    - 0.01
    - 0.001
    - 0.0001
    - 1.0e-05
    - 1.0e-06
    - 1.0e-07
  LR_PATIENCE:
    value: 5
  NODE_DROPOUT:
    values:
    - 0.0
    - 0.1
    - 0.2
    - 0.3
    - 0.4
    - 0.5
    - 0.6
    - 0.7
  SEED:
    value: 42
  TERMINATION_PATIENCE:
    value: 10
  TRANSFORMATION:
    value: dot_product
  WEIGHT_DECAY:
    values:
    - 1.0
    - 0.1
    - 0.01
    - 0.001
    - 0.0001
    - 1.0e-05
    - 1.0e-06
    - 1.0e-07
program: hps_SIGNff_DPA.py

wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_043510-253miwx5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-planet-65
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/253miwx5
wandb: Starting wandb agent \U0001f575\ufe0f
2022-06-16 04:35:18,822 - wandb.wandb_agent - INFO - Running runs: []
2022-06-16 04:35:19,080 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 04:35:19,081 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 128
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 04:35:19,089 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=128 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 04:35:24,098 - wandb.wandb_agent - INFO - Running runs: ['jlnf5u4l']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_043523-jlnf5u4l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-1
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/jlnf5u4l
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k4_dot_product_norm1_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.5917
wandb:    train_loss 1.42739
wandb: training_time 1.2073
wandb:        val_f1 0.59267
wandb:      val_loss 1.4213
wandb: 
wandb: Synced bumbling-sweep-1: https://wandb.ai/jah377/sffSHA_arxiv/runs/jlnf5u4l
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_043523-jlnf5u4l/logs
2022-06-16 04:52:14,391 - wandb.wandb_agent - INFO - Cleaning up finished run: jlnf5u4l
2022-06-16 04:52:14,726 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 04:52:14,726 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 256
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 04:52:14,734 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=256 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 04:52:19,746 - wandb.wandb_agent - INFO - Running runs: ['nxsprkej']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_045219-nxsprkej
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-2
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/nxsprkej
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k3_dot_product_norm0_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.10788
wandb:    train_loss 3.63537
wandb: training_time 0.95457
wandb:        val_f1 0.15266
wandb:      val_loss 3.62637
wandb: 
wandb: Synced valiant-sweep-2: https://wandb.ai/jah377/sffSHA_arxiv/runs/nxsprkej
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_045219-nxsprkej/logs
2022-06-16 05:07:26,947 - wandb.wandb_agent - INFO - Cleaning up finished run: nxsprkej
2022-06-16 05:07:27,334 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 05:07:27,334 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1
2022-06-16 05:07:27,344 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 05:07:32,354 - wandb.wandb_agent - INFO - Running runs: ['0r5gufvn']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_050731-0r5gufvn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-3
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/0r5gufvn
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k5_dot_product_norm1_heads1.pth

$$$ EARLY STOPPING TRIGGERED $$$
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 28
wandb:      train_f1 0.17935
wandb:    train_loss 3.64477
wandb: training_time 1.81144
wandb:        val_f1 0.07628
wandb:      val_loss 3.64739
wandb: 
wandb: Synced confused-sweep-3: https://wandb.ai/jah377/sffSHA_arxiv/runs/0r5gufvn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_050731-0r5gufvn/logs
2022-06-16 05:13:53,458 - wandb.wandb_agent - INFO - Cleaning up finished run: 0r5gufvn
2022-06-16 05:13:53,848 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 05:13:53,848 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 5
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 128
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 05:13:53,857 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=5 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=128 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 05:13:58,870 - wandb.wandb_agent - INFO - Running runs: ['i2w6fmk1']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_051359-i2w6fmk1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-4
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/i2w6fmk1
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k5_dot_product_norm0_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.1808
wandb:    train_loss 3.49212
wandb: training_time 0.85427
wandb:        val_f1 0.07796
wandb:      val_loss 3.47812
wandb: 
wandb: Synced colorful-sweep-4: https://wandb.ai/jah377/sffSHA_arxiv/runs/i2w6fmk1
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_051359-i2w6fmk1/logs
2022-06-16 05:28:19,354 - wandb.wandb_agent - INFO - Cleaning up finished run: i2w6fmk1
2022-06-16 05:28:19,759 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 05:28:19,759 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 05:28:19,766 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 05:28:24,780 - wandb.wandb_agent - INFO - Running runs: ['z7gudgc2']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_052824-z7gudgc2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-5
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/z7gudgc2
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.57424
wandb:    train_loss 1.54394
wandb: training_time 1.14508
wandb:        val_f1 0.58039
wandb:      val_loss 1.50293
wandb: 
wandb: Synced volcanic-sweep-5: https://wandb.ai/jah377/sffSHA_arxiv/runs/z7gudgc2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_052824-z7gudgc2/logs
2022-06-16 05:39:59,878 - wandb.wandb_agent - INFO - Cleaning up finished run: z7gudgc2
2022-06-16 05:40:00,283 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 05:40:00,283 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 05:40:00,290 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-16 05:40:05,304 - wandb.wandb_agent - INFO - Running runs: ['y2fvr40z']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_054005-y2fvr40z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-6
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/y2fvr40z
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.44123
wandb:    train_loss 1.97903
wandb: training_time 2.45897
wandb:        val_f1 0.46589
wandb:      val_loss 1.88583
wandb: 
wandb: Synced zany-sweep-6: https://wandb.ai/jah377/sffSHA_arxiv/runs/y2fvr40z
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_054005-y2fvr40z/logs
2022-06-16 06:00:12,080 - wandb.wandb_agent - INFO - Cleaning up finished run: y2fvr40z
2022-06-16 06:00:12,508 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:00:12,508 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 06:00:12,517 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 06:00:17,530 - wandb.wandb_agent - INFO - Running runs: ['fo8kgcf2']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_060016-fo8kgcf2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-7
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/fo8kgcf2
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.69328
wandb:    train_loss 0.9755
wandb: training_time 1.65481
wandb:        val_f1 0.6266
wandb:      val_loss 1.28803
wandb: 
wandb: Synced azure-sweep-7: https://wandb.ai/jah377/sffSHA_arxiv/runs/fo8kgcf2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_060016-fo8kgcf2/logs
2022-06-16 06:15:45,804 - wandb.wandb_agent - INFO - Cleaning up finished run: fo8kgcf2
2022-06-16 06:15:46,184 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:15:46,185 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 06:15:46,192 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 06:15:51,206 - wandb.wandb_agent - INFO - Running runs: ['n2csae2q']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_061550-n2csae2q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-8
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/n2csae2q
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k2_dot_product_norm1_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.75502
wandb:    train_loss 0.78168
wandb: training_time 1.28876
wandb:        val_f1 0.63472
wandb:      val_loss 1.24971
wandb: 
wandb: Synced devoted-sweep-8: https://wandb.ai/jah377/sffSHA_arxiv/runs/n2csae2q
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_061550-n2csae2q/logs
2022-06-16 06:32:41,432 - wandb.wandb_agent - INFO - Cleaning up finished run: n2csae2q
2022-06-16 06:32:41,841 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:32:41,841 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 06:32:41,848 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 06:32:46,861 - wandb.wandb_agent - INFO - Running runs: ['4gkzzky2']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_063247-4gkzzky2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-9
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/4gkzzky2
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm1_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.77878
wandb:    train_loss 0.71856
wandb: training_time 0.96608
wandb:        val_f1 0.57046
wandb:      val_loss 1.52244
wandb: 
wandb: Synced helpful-sweep-9: https://wandb.ai/jah377/sffSHA_arxiv/runs/4gkzzky2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_063247-4gkzzky2/logs
2022-06-16 06:47:45,046 - wandb.wandb_agent - INFO - Cleaning up finished run: 4gkzzky2
2022-06-16 06:47:45,404 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:47:45,405 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 128
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 06:47:45,413 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=128 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 06:47:50,427 - wandb.wandb_agent - INFO - Running runs: ['4wxxpb2l']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_064750-4wxxpb2l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-10
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/4wxxpb2l
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k3_dot_product_norm1_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64836
wandb:    train_loss 1.16796
wandb: training_time 1.14893
wandb:        val_f1 0.61495
wandb:      val_loss 1.27561
wandb: 
wandb: Synced lyric-sweep-10: https://wandb.ai/jah377/sffSHA_arxiv/runs/4wxxpb2l
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_064750-4wxxpb2l/logs
2022-06-16 07:03:51,136 - wandb.wandb_agent - INFO - Cleaning up finished run: 4wxxpb2l
2022-06-16 07:03:51,604 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:03:51,604 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 07:03:51,613 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 07:03:56,627 - wandb.wandb_agent - INFO - Running runs: ['ht8xf6bm']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_070356-ht8xf6bm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-11
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ht8xf6bm
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k1_dot_product_norm1_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64782
wandb:    train_loss 1.1387
wandb: training_time 1.27974
wandb:        val_f1 0.62012
wandb:      val_loss 1.25399
wandb: 
wandb: Synced lilac-sweep-11: https://wandb.ai/jah377/sffSHA_arxiv/runs/ht8xf6bm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_070356-ht8xf6bm/logs
2022-06-16 07:20:36,614 - wandb.wandb_agent - INFO - Cleaning up finished run: ht8xf6bm
2022-06-16 07:20:37,009 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:20:37,009 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 256
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 07:20:37,017 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=256 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 07:20:42,030 - wandb.wandb_agent - INFO - Running runs: ['2mctb8yv']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_072042-2mctb8yv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-12
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/2mctb8yv
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.66249
wandb:    train_loss 1.0964
wandb: training_time 1.31238
wandb:        val_f1 0.61364
wandb:      val_loss 1.31602
wandb: 
wandb: Synced leafy-sweep-12: https://wandb.ai/jah377/sffSHA_arxiv/runs/2mctb8yv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_072042-2mctb8yv/logs
2022-06-16 07:33:28,031 - wandb.wandb_agent - INFO - Cleaning up finished run: 2mctb8yv
2022-06-16 07:33:28,435 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:33:28,436 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 128
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 07:33:28,443 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=128 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 07:33:33,458 - wandb.wandb_agent - INFO - Running runs: ['dsu3djrq']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_073333-dsu3djrq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-13
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/dsu3djrq
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.36009
wandb:    train_loss 7.78504
wandb: training_time 0.83026
wandb:        val_f1 0.37897
wandb:      val_loss 6.49365
wandb: 
wandb: Synced unique-sweep-13: https://wandb.ai/jah377/sffSHA_arxiv/runs/dsu3djrq
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_073333-dsu3djrq/logs
2022-06-16 07:43:21,866 - wandb.wandb_agent - INFO - Cleaning up finished run: dsu3djrq
2022-06-16 07:43:22,442 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:43:22,443 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 07:43:22,449 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 07:43:27,462 - wandb.wandb_agent - INFO - Running runs: ['1hhpte88']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_074327-1hhpte88
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-14
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/1hhpte88
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.81425
wandb:    train_loss 0.58037
wandb: training_time 1.84927
wandb:        val_f1 0.62962
wandb:      val_loss 1.26138
wandb: 
wandb: Synced decent-sweep-14: https://wandb.ai/jah377/sffSHA_arxiv/runs/1hhpte88
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_074327-1hhpte88/logs
2022-06-16 08:00:12,905 - wandb.wandb_agent - INFO - Cleaning up finished run: 1hhpte88
2022-06-16 08:00:13,337 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 08:00:13,338 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 08:00:13,345 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 08:00:18,358 - wandb.wandb_agent - INFO - Running runs: ['8usfc17f']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_080018-8usfc17f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-15
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/8usfc17f
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.47023
wandb:    train_loss 1.89714
wandb: training_time 1.852
wandb:        val_f1 0.50018
wandb:      val_loss 1.79654
wandb: 
wandb: Synced toasty-sweep-15: https://wandb.ai/jah377/sffSHA_arxiv/runs/8usfc17f
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_080018-8usfc17f/logs
2022-06-16 08:16:58,971 - wandb.wandb_agent - INFO - Cleaning up finished run: 8usfc17f
2022-06-16 08:16:59,435 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 08:16:59,435 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 256
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 08:16:59,443 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=256 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 08:17:04,457 - wandb.wandb_agent - INFO - Running runs: ['ezjn6ceh']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_081704-ezjn6ceh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-16
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ezjn6ceh
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53823
wandb:    train_loss 1.61458
wandb: training_time 1.15842
wandb:        val_f1 0.56042
wandb:      val_loss 1.54084
wandb: 
wandb: Synced rich-sweep-16: https://wandb.ai/jah377/sffSHA_arxiv/runs/ezjn6ceh
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_081704-ezjn6ceh/logs
2022-06-16 08:28:40,018 - wandb.wandb_agent - INFO - Cleaning up finished run: ezjn6ceh
2022-06-16 08:28:40,444 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 08:28:40,444 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 08:28:40,453 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 08:28:45,466 - wandb.wandb_agent - INFO - Running runs: ['h4kib8yx']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_082844-h4kib8yx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-17
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/h4kib8yx
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.82205
wandb:    train_loss 0.54882
wandb: training_time 1.84246
wandb:        val_f1 0.62932
wandb:      val_loss 1.3275
wandb: 
wandb: Synced upbeat-sweep-17: https://wandb.ai/jah377/sffSHA_arxiv/runs/h4kib8yx
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_082844-h4kib8yx/logs
2022-06-16 08:44:21,966 - wandb.wandb_agent - INFO - Cleaning up finished run: h4kib8yx
2022-06-16 08:44:22,514 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 08:44:22,515 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 08:44:22,522 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 08:44:27,534 - wandb.wandb_agent - INFO - Running runs: ['0wmal911']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_084427-0wmal911
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-18
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/0wmal911
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.77617
wandb:    train_loss 0.70039
wandb: training_time 2.33698
wandb:        val_f1 0.6269
wandb:      val_loss 1.29413
wandb: 
wandb: Synced efficient-sweep-18: https://wandb.ai/jah377/sffSHA_arxiv/runs/0wmal911
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_084427-0wmal911/logs
2022-06-16 09:04:22,999 - wandb.wandb_agent - INFO - Cleaning up finished run: 0wmal911
2022-06-16 09:04:23,440 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:04:23,441 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 09:04:23,448 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 09:04:28,462 - wandb.wandb_agent - INFO - Running runs: ['i93ueh9o']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_090427-i93ueh9o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-19
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/i93ueh9o
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.26319
wandb:    train_loss 3.72134
wandb: training_time 2.63879
wandb:        val_f1 0.27004
wandb:      val_loss 4.74581
wandb: 
wandb: Synced icy-sweep-19: https://wandb.ai/jah377/sffSHA_arxiv/runs/i93ueh9o
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_090427-i93ueh9o/logs
2022-06-16 09:26:04,392 - wandb.wandb_agent - INFO - Cleaning up finished run: i93ueh9o
2022-06-16 09:26:04,843 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:26:04,844 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 09:26:04,850 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 09:26:09,865 - wandb.wandb_agent - INFO - Running runs: ['5f1yc3l5']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_092610-5f1yc3l5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-20
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/5f1yc3l5
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.7182
wandb:    train_loss 0.91195
wandb: training_time 1.33754
wandb:        val_f1 0.63747
wandb:      val_loss 1.20541
wandb: 
wandb: Synced volcanic-sweep-20: https://wandb.ai/jah377/sffSHA_arxiv/runs/5f1yc3l5
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_092610-5f1yc3l5/logs
2022-06-16 09:38:53,588 - wandb.wandb_agent - INFO - Cleaning up finished run: 5f1yc3l5
2022-06-16 09:38:54,162 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:38:54,162 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 09:38:54,169 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 09:38:59,182 - wandb.wandb_agent - INFO - Running runs: ['b1jta5z6']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_093859-b1jta5z6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-21
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/b1jta5z6
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.6076
wandb:    train_loss 1.34187
wandb: training_time 1.3227
wandb:        val_f1 0.60599
wandb:      val_loss 1.37249
wandb: 
wandb: Synced peachy-sweep-21: https://wandb.ai/jah377/sffSHA_arxiv/runs/b1jta5z6
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_093859-b1jta5z6/logs
2022-06-16 09:52:21,181 - wandb.wandb_agent - INFO - Cleaning up finished run: b1jta5z6
2022-06-16 09:52:21,655 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:52:21,655 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 09:52:21,664 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 09:52:26,674 - wandb.wandb_agent - INFO - Running runs: ['zud7tam6']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_095226-zud7tam6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-22
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/zud7tam6
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.71108
wandb:    train_loss 0.90313
wandb: training_time 1.58316
wandb:        val_f1 0.63552
wandb:      val_loss 1.21989
wandb: 
wandb: Synced vital-sweep-22: https://wandb.ai/jah377/sffSHA_arxiv/runs/zud7tam6
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_095226-zud7tam6/logs
2022-06-16 10:07:23,877 - wandb.wandb_agent - INFO - Cleaning up finished run: zud7tam6
2022-06-16 10:07:24,619 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:07:24,619 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 10:07:24,626 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 10:07:29,638 - wandb.wandb_agent - INFO - Running runs: ['cuozl564']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_100729-cuozl564
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-23
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/cuozl564
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
$$$ EARLY STOPPING TRIGGERED $$$
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 22
wandb:      train_f1 0.96553
wandb:    train_loss 0.18456
wandb: training_time 1.41645
wandb:        val_f1 0.58599
wandb:      val_loss 1.86773
wandb: 
wandb: Synced earnest-sweep-23: https://wandb.ai/jah377/sffSHA_arxiv/runs/cuozl564
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_100729-cuozl564/logs
2022-06-16 10:08:52,131 - wandb.wandb_agent - INFO - Cleaning up finished run: cuozl564
2022-06-16 10:08:52,936 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:08:52,936 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 10:08:52,945 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 10:08:57,959 - wandb.wandb_agent - INFO - Running runs: ['4gewkw52']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_100858-4gewkw52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-24
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/4gewkw52
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53921
wandb:    train_loss 1.62809
wandb: training_time 2.05882
wandb:        val_f1 0.54901
wandb:      val_loss 1.59912
wandb: 
wandb: Synced eager-sweep-24: https://wandb.ai/jah377/sffSHA_arxiv/runs/4gewkw52
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_100858-4gewkw52/logs
2022-06-16 10:26:44,898 - wandb.wandb_agent - INFO - Cleaning up finished run: 4gewkw52
2022-06-16 10:26:45,323 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:26:45,324 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 10:26:45,331 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 10:26:50,346 - wandb.wandb_agent - INFO - Running runs: ['lo1nrf2u']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_102650-lo1nrf2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-25
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/lo1nrf2u
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53822
wandb:    train_loss 1.60438
wandb: training_time 1.37905
wandb:        val_f1 0.55133
wandb:      val_loss 1.57332
wandb: 
wandb: Synced sunny-sweep-25: https://wandb.ai/jah377/sffSHA_arxiv/runs/lo1nrf2u
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_102650-lo1nrf2u/logs
2022-06-16 10:39:59,705 - wandb.wandb_agent - INFO - Cleaning up finished run: lo1nrf2u
2022-06-16 10:40:00,102 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:40:00,102 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 10:40:00,109 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 10:40:05,124 - wandb.wandb_agent - INFO - Running runs: ['e54jgp01']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_104005-e54jgp01
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-26
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/e54jgp01
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.74002
wandb:    train_loss 0.86257
wandb: training_time 1.64669
wandb:        val_f1 0.63727
wandb:      val_loss 1.20361
wandb: 
wandb: Synced comic-sweep-26: https://wandb.ai/jah377/sffSHA_arxiv/runs/e54jgp01
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_104005-e54jgp01/logs
2022-06-16 10:55:28,934 - wandb.wandb_agent - INFO - Cleaning up finished run: e54jgp01
2022-06-16 10:55:29,401 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:55:29,401 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 10:55:29,407 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 10:55:34,418 - wandb.wandb_agent - INFO - Running runs: ['4iiq8d3h']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_105533-4iiq8d3h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-27
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/4iiq8d3h
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.73121
wandb:    train_loss 0.88626
wandb: training_time 1.75153
wandb:        val_f1 0.63616
wandb:      val_loss 1.22021
wandb: 
wandb: Synced vivid-sweep-27: https://wandb.ai/jah377/sffSHA_arxiv/runs/4iiq8d3h
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_105533-4iiq8d3h/logs
2022-06-16 11:12:28,403 - wandb.wandb_agent - INFO - Cleaning up finished run: 4iiq8d3h
2022-06-16 11:12:28,785 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 11:12:28,785 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 11:12:28,793 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 11:12:33,808 - wandb.wandb_agent - INFO - Running runs: ['ssw79end']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_111233-ssw79end
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-28
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ssw79end
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.47998
wandb:    train_loss 1.82254
wandb: training_time 1.23679
wandb:        val_f1 0.5029
wandb:      val_loss 1.73043
wandb: 
wandb: Synced divine-sweep-28: https://wandb.ai/jah377/sffSHA_arxiv/runs/ssw79end
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_111233-ssw79end/logs
2022-06-16 11:24:51,105 - wandb.wandb_agent - INFO - Cleaning up finished run: ssw79end
2022-06-16 11:24:51,621 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 11:24:51,622 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 11:24:51,631 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 11:24:56,645 - wandb.wandb_agent - INFO - Running runs: ['wwt9epm3']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_112456-wwt9epm3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-29
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/wwt9epm3
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.6128
wandb:    train_loss 1.30488
wandb: training_time 1.51789
wandb:        val_f1 0.61495
wandb:      val_loss 1.30506
wandb: 
wandb: Synced resilient-sweep-29: https://wandb.ai/jah377/sffSHA_arxiv/runs/wwt9epm3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_112456-wwt9epm3/logs
2022-06-16 11:39:13,661 - wandb.wandb_agent - INFO - Cleaning up finished run: wwt9epm3
2022-06-16 11:39:14,100 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 11:39:14,100 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 11:39:14,109 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 11:39:19,123 - wandb.wandb_agent - INFO - Running runs: ['2del5oe7']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_113919-2del5oe7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-30
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/2del5oe7
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.58513
wandb:    train_loss 1.44202
wandb: training_time 1.78994
wandb:        val_f1 0.59281
wandb:      val_loss 1.4481
wandb: 
wandb: Synced crisp-sweep-30: https://wandb.ai/jah377/sffSHA_arxiv/runs/2del5oe7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_113919-2del5oe7/logs
2022-06-16 11:55:40,756 - wandb.wandb_agent - INFO - Cleaning up finished run: 2del5oe7
2022-06-16 11:55:41,177 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 11:55:41,177 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 256
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-16 11:55:41,186 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=256 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
2022-06-16 11:55:46,200 - wandb.wandb_agent - INFO - Running runs: ['nfud462m']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_115546-nfud462m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-31
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/nfud462m
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.61511
wandb:    train_loss 1.35283
wandb: training_time 1.81772
wandb:        val_f1 0.60495
wandb:      val_loss 1.38037
wandb: 
wandb: Synced generous-sweep-31: https://wandb.ai/jah377/sffSHA_arxiv/runs/nfud462m
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_115546-nfud462m/logs
2022-06-16 12:11:29,642 - wandb.wandb_agent - INFO - Cleaning up finished run: nfud462m
2022-06-16 12:11:30,063 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:11:30,063 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 12:11:30,072 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 12:11:35,087 - wandb.wandb_agent - INFO - Running runs: ['phqn3o31']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_121135-phqn3o31
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-32
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/phqn3o31
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.54449
wandb:    train_loss 1.77609
wandb: training_time 1.51724
wandb:        val_f1 0.56425
wandb:      val_loss 1.7214
wandb: 
wandb: Synced balmy-sweep-32: https://wandb.ai/jah377/sffSHA_arxiv/runs/phqn3o31
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_121135-phqn3o31/logs
2022-06-16 12:27:09,126 - wandb.wandb_agent - INFO - Cleaning up finished run: phqn3o31
2022-06-16 12:27:09,621 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:27:09,621 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-16 12:27:09,628 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
2022-06-16 12:27:14,642 - wandb.wandb_agent - INFO - Running runs: ['onhitwyr']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_122714-onhitwyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-33
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/onhitwyr
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.52376
wandb:    train_loss 1.75061
wandb: training_time 1.15149
wandb:        val_f1 0.53777
wandb:      val_loss 1.71307
wandb: 
wandb: Synced lucky-sweep-33: https://wandb.ai/jah377/sffSHA_arxiv/runs/onhitwyr
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_122714-onhitwyr/logs
2022-06-16 12:39:05,750 - wandb.wandb_agent - INFO - Cleaning up finished run: onhitwyr
2022-06-16 12:39:06,454 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:39:06,455 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 1
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 12:39:06,464 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=1 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 12:39:11,478 - wandb.wandb_agent - INFO - Running runs: ['ybhiqzck']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_123911-ybhiqzck
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-34
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ybhiqzck
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.79056
wandb:    train_loss 0.68147
wandb: training_time 1.40346
wandb:        val_f1 0.6374
wandb:      val_loss 1.21062
wandb: 
wandb: Synced effortless-sweep-34: https://wandb.ai/jah377/sffSHA_arxiv/runs/ybhiqzck
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_123911-ybhiqzck/logs
2022-06-16 12:52:20,622 - wandb.wandb_agent - INFO - Cleaning up finished run: ybhiqzck
2022-06-16 12:52:21,311 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:52:21,311 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 256
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 12:52:21,318 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=256 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 12:52:26,332 - wandb.wandb_agent - INFO - Running runs: ['pbw5lqmh']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_125226-pbw5lqmh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-35
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/pbw5lqmh
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.54953
wandb:    train_loss 1.59418
wandb: training_time 1.98714
wandb:        val_f1 0.56794
wandb:      val_loss 1.52709
wandb: 
wandb: Synced rare-sweep-35: https://wandb.ai/jah377/sffSHA_arxiv/runs/pbw5lqmh
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_125226-pbw5lqmh/logs
2022-06-16 13:08:25,538 - wandb.wandb_agent - INFO - Cleaning up finished run: pbw5lqmh
2022-06-16 13:08:25,990 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:08:25,991 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 13:08:26,000 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 13:08:31,014 - wandb.wandb_agent - INFO - Running runs: ['5t231tsp']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_130831-5t231tsp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-36
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/5t231tsp
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.68228
wandb:    train_loss 1.04823
wandb: training_time 1.48904
wandb:        val_f1 0.62707
wandb:      val_loss 1.24062
wandb: 
wandb: Synced zany-sweep-36: https://wandb.ai/jah377/sffSHA_arxiv/runs/5t231tsp
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_130831-5t231tsp/logs
2022-06-16 13:23:25,041 - wandb.wandb_agent - INFO - Cleaning up finished run: 5t231tsp
2022-06-16 13:23:25,571 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:23:25,572 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1
2022-06-16 13:23:25,579 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1
Using backend: pytorch
2022-06-16 13:23:30,595 - wandb.wandb_agent - INFO - Running runs: ['vroxopqk']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_132330-vroxopqk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-37
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/vroxopqk
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.4817
wandb:    train_loss 2.04242
wandb: training_time 1.56983
wandb:        val_f1 0.50421
wandb:      val_loss 1.96276
wandb: 
wandb: Synced good-sweep-37: https://wandb.ai/jah377/sffSHA_arxiv/runs/vroxopqk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_132330-vroxopqk/logs
2022-06-16 13:37:22,854 - wandb.wandb_agent - INFO - Cleaning up finished run: vroxopqk
2022-06-16 13:37:23,409 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:37:23,409 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 256
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 13:37:23,417 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=256 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 13:37:28,430 - wandb.wandb_agent - INFO - Running runs: ['myol6j0n']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_133728-myol6j0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-38
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/myol6j0n
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.57423
wandb:    train_loss 1.45163
wandb: training_time 1.41004
wandb:        val_f1 0.58569
wandb:      val_loss 1.41856
wandb: 
wandb: Synced warm-sweep-38: https://wandb.ai/jah377/sffSHA_arxiv/runs/myol6j0n
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_133728-myol6j0n/logs
2022-06-16 13:51:24,510 - wandb.wandb_agent - INFO - Cleaning up finished run: myol6j0n
2022-06-16 13:51:24,984 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:51:24,984 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 13:51:24,990 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 13:51:30,005 - wandb.wandb_agent - INFO - Running runs: ['4xh5xlil']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_135130-4xh5xlil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-39
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/4xh5xlil
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.66258
wandb:    train_loss 1.08324
wandb: training_time 0.94132
wandb:        val_f1 0.58086
wandb:      val_loss 1.41456
wandb: 
wandb: Synced solar-sweep-39: https://wandb.ai/jah377/sffSHA_arxiv/runs/4xh5xlil
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_135130-4xh5xlil/logs
2022-06-16 14:01:39,010 - wandb.wandb_agent - INFO - Cleaning up finished run: 4xh5xlil
2022-06-16 14:01:39,445 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:01:39,445 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 14:01:39,455 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 14:01:44,466 - wandb.wandb_agent - INFO - Running runs: ['lia888s3']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_140144-lia888s3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-40
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/lia888s3
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.72279
wandb:    train_loss 0.8972
wandb: training_time 1.05004
wandb:        val_f1 0.5873
wandb:      val_loss 1.404
wandb: 
wandb: Synced worthy-sweep-40: https://wandb.ai/jah377/sffSHA_arxiv/runs/lia888s3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_140144-lia888s3/logs
2022-06-16 14:12:44,555 - wandb.wandb_agent - INFO - Cleaning up finished run: lia888s3
2022-06-16 14:12:45,167 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:12:45,167 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 14:12:45,176 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 14:12:50,190 - wandb.wandb_agent - INFO - Running runs: ['4vn2zrsx']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_141250-4vn2zrsx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-41
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/4vn2zrsx
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm0_heads1.pth

$$$ EARLY STOPPING TRIGGERED $$$
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 33
wandb:      train_f1 0.81499
wandb:    train_loss 0.57193
wandb: training_time 1.09155
wandb:        val_f1 0.57787
wandb:      val_loss 1.62899
wandb: 
wandb: Synced efficient-sweep-41: https://wandb.ai/jah377/sffSHA_arxiv/runs/4vn2zrsx
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_141250-4vn2zrsx/logs
2022-06-16 14:18:25,649 - wandb.wandb_agent - INFO - Cleaning up finished run: 4vn2zrsx
2022-06-16 14:18:26,212 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:18:26,213 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 14:18:26,220 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 14:18:31,234 - wandb.wandb_agent - INFO - Running runs: ['za6lfqmm']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_141830-za6lfqmm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-42
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/za6lfqmm
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.3734
wandb:    train_loss 2.66052
wandb: training_time 2.7841
wandb:        val_f1 0.37454
wandb:      val_loss 2.60837
wandb: 
wandb: Synced vibrant-sweep-42: https://wandb.ai/jah377/sffSHA_arxiv/runs/za6lfqmm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_141830-za6lfqmm/logs
2022-06-16 14:40:53,010 - wandb.wandb_agent - INFO - Cleaning up finished run: za6lfqmm
2022-06-16 14:40:53,601 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:40:53,601 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 1
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 14:40:53,609 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=1 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 14:40:58,624 - wandb.wandb_agent - INFO - Running runs: ['i9dmtqxn']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_144058-i9dmtqxn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-43
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/i9dmtqxn
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.81691
wandb:    train_loss 0.58862
wandb: training_time 1.23947
wandb:        val_f1 0.63925
wandb:      val_loss 1.2396
wandb: 
wandb: Synced wild-sweep-43: https://wandb.ai/jah377/sffSHA_arxiv/runs/i9dmtqxn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_144058-i9dmtqxn/logs
2022-06-16 14:53:32,902 - wandb.wandb_agent - INFO - Cleaning up finished run: i9dmtqxn
2022-06-16 14:53:33,450 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:53:33,451 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 128
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 14:53:33,458 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=128 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 14:53:38,473 - wandb.wandb_agent - INFO - Running runs: ['v4e3b5wn']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_145338-v4e3b5wn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-44
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/v4e3b5wn
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.46438
wandb:    train_loss 2.05441
wandb: training_time 1.48662
wandb:        val_f1 0.49639
wandb:      val_loss 1.95171
wandb: 
wandb: Synced atomic-sweep-44: https://wandb.ai/jah377/sffSHA_arxiv/runs/v4e3b5wn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_145338-v4e3b5wn/logs
2022-06-16 15:08:44,830 - wandb.wandb_agent - INFO - Cleaning up finished run: v4e3b5wn
2022-06-16 15:08:45,367 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:08:45,367 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 5
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-16 15:08:45,375 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=5 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
2022-06-16 15:08:50,390 - wandb.wandb_agent - INFO - Running runs: ['ngd10gp2']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_150850-ngd10gp2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-45
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ngd10gp2
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.7543
wandb:    train_loss 0.86172
wandb: training_time 1.79267
wandb:        val_f1 0.63888
wandb:      val_loss 1.23229
wandb: 
wandb: Synced polished-sweep-45: https://wandb.ai/jah377/sffSHA_arxiv/runs/ngd10gp2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_150850-ngd10gp2/logs
2022-06-16 15:25:08,326 - wandb.wandb_agent - INFO - Cleaning up finished run: ngd10gp2
2022-06-16 15:25:08,859 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:25:08,859 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 15:25:08,867 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 15:25:13,878 - wandb.wandb_agent - INFO - Running runs: ['1f4alo7u']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_152514-1f4alo7u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-46
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/1f4alo7u
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
$$$ EARLY STOPPING TRIGGERED $$$
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 21
wandb:      train_f1 0.99956
wandb:    train_loss 0.01062
wandb: training_time 2.56098
wandb:        val_f1 0.5869
wandb:      val_loss 2.21397
wandb: 
wandb: Synced elated-sweep-46: https://wandb.ai/jah377/sffSHA_arxiv/runs/1f4alo7u
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_152514-1f4alo7u/logs
2022-06-16 15:26:56,949 - wandb.wandb_agent - INFO - Cleaning up finished run: 1f4alo7u
2022-06-16 15:26:57,410 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:26:57,411 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 15:26:57,417 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 15:27:02,430 - wandb.wandb_agent - INFO - Running runs: ['ri5z7qkl']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_152702-ri5z7qkl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-47
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ri5z7qkl
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.54221
wandb:    train_loss 1.5561
wandb: training_time 1.21749
wandb:        val_f1 0.54394
wandb:      val_loss 1.53989
wandb: 
wandb: Synced genial-sweep-47: https://wandb.ai/jah377/sffSHA_arxiv/runs/ri5z7qkl
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_152702-ri5z7qkl/logs
2022-06-16 15:39:16,238 - wandb.wandb_agent - INFO - Cleaning up finished run: ri5z7qkl
2022-06-16 15:39:16,849 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:39:16,849 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 15:39:16,856 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 15:39:21,870 - wandb.wandb_agent - INFO - Running runs: ['82m7j6aq']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_153922-82m7j6aq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-48
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/82m7j6aq
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64778
wandb:    train_loss 1.19007
wandb: training_time 1.16746
wandb:        val_f1 0.62304
wandb:      val_loss 1.26503
wandb: 
wandb: Synced neat-sweep-48: https://wandb.ai/jah377/sffSHA_arxiv/runs/82m7j6aq
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_153922-82m7j6aq/logs
2022-06-16 15:52:06,132 - wandb.wandb_agent - INFO - Cleaning up finished run: 82m7j6aq
2022-06-16 15:52:06,716 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:52:06,716 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-16 15:52:06,723 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
2022-06-16 15:52:11,734 - wandb.wandb_agent - INFO - Running runs: ['p8uzt6ip']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_155211-p8uzt6ip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-49
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/p8uzt6ip
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53398
wandb:    train_loss 1.62576
wandb: training_time 1.36303
wandb:        val_f1 0.55472
wandb:      val_loss 1.56117
wandb: 
wandb: Synced true-sweep-49: https://wandb.ai/jah377/sffSHA_arxiv/runs/p8uzt6ip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_155211-p8uzt6ip/logs
2022-06-16 16:06:41,696 - wandb.wandb_agent - INFO - Cleaning up finished run: p8uzt6ip
2022-06-16 16:06:42,262 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:06:42,263 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 5
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1
2022-06-16 16:06:42,270 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=5 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1
Using backend: pytorch
2022-06-16 16:06:47,284 - wandb.wandb_agent - INFO - Running runs: ['uxvl6nxs']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_160647-uxvl6nxs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-50
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/uxvl6nxs
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 1, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
$$$ EARLY STOPPING TRIGGERED $$$
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 11
wandb:      train_f1 0.0451
wandb:    train_loss 3.6925
wandb: training_time 1.58501
wandb:        val_f1 0.02403
wandb:      val_loss 3.74309
wandb: 
wandb: Synced comic-sweep-50: https://wandb.ai/jah377/sffSHA_arxiv/runs/uxvl6nxs
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_160647-uxvl6nxs/logs
2022-06-16 16:07:38,886 - wandb.wandb_agent - INFO - Cleaning up finished run: uxvl6nxs
2022-06-16 16:07:39,603 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:07:39,604 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 256
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 16:07:39,610 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=256 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 16:07:44,625 - wandb.wandb_agent - INFO - Running runs: ['5kpvrbw9']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_160743-5kpvrbw9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-51
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/5kpvrbw9
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.68772
wandb:    train_loss 1.00611
wandb: training_time 1.96646
wandb:        val_f1 0.63328
wandb:      val_loss 1.21137
wandb: 
wandb: Synced smart-sweep-51: https://wandb.ai/jah377/sffSHA_arxiv/runs/5kpvrbw9
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_160743-5kpvrbw9/logs
2022-06-16 16:25:47,684 - wandb.wandb_agent - INFO - Cleaning up finished run: 5kpvrbw9
2022-06-16 16:25:48,245 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:25:48,246 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 16:25:48,253 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 16:25:53,266 - wandb.wandb_agent - INFO - Running runs: ['ro0gypku']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_162553-ro0gypku
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-52
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ro0gypku
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.62947
wandb:    train_loss 1.23414
wandb: training_time 0.88687
wandb:        val_f1 0.62012
wandb:      val_loss 1.26211
wandb: 
wandb: Synced volcanic-sweep-52: https://wandb.ai/jah377/sffSHA_arxiv/runs/ro0gypku
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_162553-ro0gypku/logs
2022-06-16 16:36:13,881 - wandb.wandb_agent - INFO - Cleaning up finished run: ro0gypku
2022-06-16 16:36:14,407 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:36:14,407 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 16:36:14,416 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 16:36:19,430 - wandb.wandb_agent - INFO - Running runs: ['s2rp5817']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_163619-s2rp5817
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-53
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/s2rp5817
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.79185
wandb:    train_loss 0.64727
wandb: training_time 2.31205
wandb:        val_f1 0.63952
wandb:      val_loss 1.2353
wandb: 
wandb: Synced dainty-sweep-53: https://wandb.ai/jah377/sffSHA_arxiv/runs/s2rp5817
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_163619-s2rp5817/logs
2022-06-16 16:56:09,678 - wandb.wandb_agent - INFO - Cleaning up finished run: s2rp5817
2022-06-16 16:56:10,368 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:56:10,368 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 16:56:10,374 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 16:56:15,386 - wandb.wandb_agent - INFO - Running runs: ['fuxtoamo']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_165615-fuxtoamo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-54
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/fuxtoamo
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.60126
wandb:    train_loss 1.35228
wandb: training_time 2.46507
wandb:        val_f1 0.60556
wandb:      val_loss 1.35937
wandb: 
wandb: Synced firm-sweep-54: https://wandb.ai/jah377/sffSHA_arxiv/runs/fuxtoamo
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_165615-fuxtoamo/logs
2022-06-16 17:18:03,010 - wandb.wandb_agent - INFO - Cleaning up finished run: fuxtoamo
2022-06-16 17:18:03,529 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 17:18:03,529 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 17:18:03,538 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 17:18:08,551 - wandb.wandb_agent - INFO - Running runs: ['87rmkm3w']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_171808-87rmkm3w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-55
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/87rmkm3w
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.59872
wandb:    train_loss 1.36459
wandb: training_time 1.05315
wandb:        val_f1 0.60197
wandb:      val_loss 1.34
wandb: 
wandb: Synced happy-sweep-55: https://wandb.ai/jah377/sffSHA_arxiv/runs/87rmkm3w
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_171808-87rmkm3w/logs
2022-06-16 17:29:56,201 - wandb.wandb_agent - INFO - Cleaning up finished run: 87rmkm3w
2022-06-16 17:29:56,746 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 17:29:56,747 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 17:29:56,754 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 17:30:01,768 - wandb.wandb_agent - INFO - Running runs: ['24cwuzu0']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_173002-24cwuzu0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-56
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/24cwuzu0
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.60348
wandb:    train_loss 1.32419
wandb: training_time 1.47168
wandb:        val_f1 0.60707
wandb:      val_loss 1.31318
wandb: 
wandb: Synced wobbly-sweep-56: https://wandb.ai/jah377/sffSHA_arxiv/runs/24cwuzu0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_173002-24cwuzu0/logs
2022-06-16 17:43:43,599 - wandb.wandb_agent - INFO - Cleaning up finished run: 24cwuzu0
2022-06-16 17:43:44,184 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 17:43:44,184 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 17:43:44,192 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 17:43:49,207 - wandb.wandb_agent - INFO - Running runs: ['csj50wd8']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_174349-csj50wd8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-57
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/csj50wd8
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.70421
wandb:    train_loss 0.9314
wandb: training_time 1.5293
wandb:        val_f1 0.63099
wandb:      val_loss 1.23387
wandb: 
wandb: Synced wandering-sweep-57: https://wandb.ai/jah377/sffSHA_arxiv/runs/csj50wd8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_174349-csj50wd8/logs
2022-06-16 17:58:49,214 - wandb.wandb_agent - INFO - Cleaning up finished run: csj50wd8
2022-06-16 17:58:49,789 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 17:58:49,789 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 4
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 17:58:49,796 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=4 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 17:58:54,810 - wandb.wandb_agent - INFO - Running runs: ['av4c6mnl']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_175854-av4c6mnl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-58
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/av4c6mnl
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.73465
wandb:    train_loss 0.85111
wandb: training_time 1.08933
wandb:        val_f1 0.62969
wandb:      val_loss 1.23137
wandb: 
wandb: Synced giddy-sweep-58: https://wandb.ai/jah377/sffSHA_arxiv/runs/av4c6mnl
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_175854-av4c6mnl/logs
2022-06-16 18:11:07,478 - wandb.wandb_agent - INFO - Cleaning up finished run: av4c6mnl
2022-06-16 18:11:08,026 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:11:08,027 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 18:11:08,035 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 18:11:13,051 - wandb.wandb_agent - INFO - Running runs: ['wr4hxmv5']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_181113-wr4hxmv5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-59
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/wr4hxmv5
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65583
wandb:    train_loss 1.14168
wandb: training_time 2.24714
wandb:        val_f1 0.6316
wandb:      val_loss 1.23007
wandb: 
wandb: Synced driven-sweep-59: https://wandb.ai/jah377/sffSHA_arxiv/runs/wr4hxmv5
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_181113-wr4hxmv5/logs
2022-06-16 18:30:26,702 - wandb.wandb_agent - INFO - Cleaning up finished run: wr4hxmv5
2022-06-16 18:30:27,247 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:30:27,248 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 18:30:27,255 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 18:30:32,270 - wandb.wandb_agent - INFO - Running runs: ['gryjth9e']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_183032-gryjth9e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-60
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/gryjth9e
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65795
wandb:    train_loss 1.11963
wandb: training_time 1.40555
wandb:        val_f1 0.63046
wandb:      val_loss 1.22732
wandb: 
wandb: Synced rural-sweep-60: https://wandb.ai/jah377/sffSHA_arxiv/runs/gryjth9e
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_183032-gryjth9e/logs
2022-06-16 18:44:14,166 - wandb.wandb_agent - INFO - Cleaning up finished run: gryjth9e
2022-06-16 18:44:14,753 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:44:14,754 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 18:44:14,761 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 18:44:19,776 - wandb.wandb_agent - INFO - Running runs: ['pfpq0vnm']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_184419-pfpq0vnm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-61
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/pfpq0vnm
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64642
wandb:    train_loss 1.19361
wandb: training_time 0.85521
wandb:        val_f1 0.62163
wandb:      val_loss 1.28196
wandb: 
wandb: Synced visionary-sweep-61: https://wandb.ai/jah377/sffSHA_arxiv/runs/pfpq0vnm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_184419-pfpq0vnm/logs
2022-06-16 18:53:49,328 - wandb.wandb_agent - INFO - Cleaning up finished run: pfpq0vnm
2022-06-16 18:53:50,037 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:53:50,037 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 18:53:50,044 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 18:53:55,058 - wandb.wandb_agent - INFO - Running runs: ['9mn6f2sm']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_185354-9mn6f2sm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-62
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/9mn6f2sm
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.655
wandb:    train_loss 1.13319
wandb: training_time 2.04145
wandb:        val_f1 0.62871
wandb:      val_loss 1.27185
wandb: 
wandb: Synced clean-sweep-62: https://wandb.ai/jah377/sffSHA_arxiv/runs/9mn6f2sm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_185354-9mn6f2sm/logs
2022-06-16 19:12:39,868 - wandb.wandb_agent - INFO - Cleaning up finished run: 9mn6f2sm
2022-06-16 19:12:40,659 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:12:40,660 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 19:12:40,665 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 19:12:45,678 - wandb.wandb_agent - INFO - Running runs: ['m60795x6']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_191245-m60795x6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-63
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/m60795x6
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.62458
wandb:    train_loss 1.26882
wandb: training_time 0.91181
wandb:        val_f1 0.6126
wandb:      val_loss 1.31282
wandb: 
wandb: Synced sandy-sweep-63: https://wandb.ai/jah377/sffSHA_arxiv/runs/m60795x6
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_191245-m60795x6/logs
2022-06-16 19:23:10,600 - wandb.wandb_agent - INFO - Cleaning up finished run: m60795x6
2022-06-16 19:23:11,219 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:23:11,220 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 19:23:11,226 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-16 19:23:16,238 - wandb.wandb_agent - INFO - Running runs: ['3hmpjsds']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_192316-3hmpjsds
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-64
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/3hmpjsds
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.6731
wandb:    train_loss 1.0949
wandb: training_time 1.30921
wandb:        val_f1 0.63391
wandb:      val_loss 1.22249
wandb: 
wandb: Synced stellar-sweep-64: https://wandb.ai/jah377/sffSHA_arxiv/runs/3hmpjsds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_192316-3hmpjsds/logs
2022-06-16 19:36:51,511 - wandb.wandb_agent - INFO - Cleaning up finished run: 3hmpjsds
2022-06-16 19:36:52,287 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:36:52,287 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 5
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 19:36:52,294 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=5 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 19:36:57,308 - wandb.wandb_agent - INFO - Running runs: ['loqzciku']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_193657-loqzciku
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-65
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/loqzciku
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.57075
wandb:    train_loss 1.46257
wandb: training_time 1.41821
wandb:        val_f1 0.58515
wandb:      val_loss 1.41308
wandb: 
wandb: Synced quiet-sweep-65: https://wandb.ai/jah377/sffSHA_arxiv/runs/loqzciku
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_193657-loqzciku/logs
2022-06-16 19:51:27,241 - wandb.wandb_agent - INFO - Cleaning up finished run: loqzciku
2022-06-16 19:51:27,786 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:51:27,786 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 19:51:27,793 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 19:51:32,807 - wandb.wandb_agent - INFO - Running runs: ['hza9fd3i']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_195132-hza9fd3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-66
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/hza9fd3i
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.18708
wandb:    train_loss 3.66637
wandb: training_time 0.92893
wandb:        val_f1 0.08698
wandb:      val_loss 3.56739
wandb: 
wandb: Synced deep-sweep-66: https://wandb.ai/jah377/sffSHA_arxiv/runs/hza9fd3i
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_195132-hza9fd3i/logs
2022-06-16 20:02:28,571 - wandb.wandb_agent - INFO - Cleaning up finished run: hza9fd3i
2022-06-16 20:02:29,102 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:02:29,102 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 20:02:29,109 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-16 20:02:34,123 - wandb.wandb_agent - INFO - Running runs: ['8bt9akzs']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_200234-8bt9akzs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-67
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/8bt9akzs
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.06583
wandb:    train_loss 3.657
wandb: training_time 0.72511
wandb:        val_f1 0.0444
wandb:      val_loss 3.67244
wandb: 
wandb: Synced visionary-sweep-67: https://wandb.ai/jah377/sffSHA_arxiv/runs/8bt9akzs
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_200234-8bt9akzs/logs
2022-06-16 20:11:41,893 - wandb.wandb_agent - INFO - Cleaning up finished run: 8bt9akzs
2022-06-16 20:11:42,528 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:11:42,528 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 20:11:42,534 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 20:11:47,548 - wandb.wandb_agent - INFO - Running runs: ['gjrkqtf5']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_201147-gjrkqtf5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-68
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/gjrkqtf5
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.57865
wandb:    train_loss 1.59892
wandb: training_time 0.87261
wandb:        val_f1 0.58962
wandb:      val_loss 1.56116
wandb: 
wandb: Synced electric-sweep-68: https://wandb.ai/jah377/sffSHA_arxiv/runs/gjrkqtf5
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_201147-gjrkqtf5/logs
2022-06-16 20:22:17,197 - wandb.wandb_agent - INFO - Cleaning up finished run: gjrkqtf5
2022-06-16 20:22:17,663 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:22:17,664 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 20:22:17,671 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 20:22:22,686 - wandb.wandb_agent - INFO - Running runs: ['klajr91v']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_202222-klajr91v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-69
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/klajr91v
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.792
wandb:    train_loss 0.7063
wandb: training_time 0.88039
wandb:        val_f1 0.58334
wandb:      val_loss 1.43998
wandb: 
wandb: Synced neat-sweep-69: https://wandb.ai/jah377/sffSHA_arxiv/runs/klajr91v
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_202222-klajr91v/logs
2022-06-16 20:32:05,768 - wandb.wandb_agent - INFO - Cleaning up finished run: klajr91v
2022-06-16 20:32:06,253 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:32:06,253 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 20:32:06,261 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 20:32:11,274 - wandb.wandb_agent - INFO - Running runs: ['805i7x9s']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_203211-805i7x9s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-70
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/805i7x9s
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65352
wandb:    train_loss 1.15002
wandb: training_time 1.00495
wandb:        val_f1 0.6274
wandb:      val_loss 1.25411
wandb: 
wandb: Synced decent-sweep-70: https://wandb.ai/jah377/sffSHA_arxiv/runs/805i7x9s
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_203211-805i7x9s/logs
2022-06-16 20:43:23,354 - wandb.wandb_agent - INFO - Cleaning up finished run: 805i7x9s
2022-06-16 20:43:24,150 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:43:24,151 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 20:43:24,158 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 20:43:29,173 - wandb.wandb_agent - INFO - Running runs: ['aed69mlz']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_204329-aed69mlz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-71
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/aed69mlz
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.5857
wandb:    train_loss 1.48838
wandb: training_time 0.98154
wandb:        val_f1 0.59153
wandb:      val_loss 1.45525
wandb: 
wandb: Synced expert-sweep-71: https://wandb.ai/jah377/sffSHA_arxiv/runs/aed69mlz
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_204329-aed69mlz/logs
2022-06-16 20:54:26,165 - wandb.wandb_agent - INFO - Cleaning up finished run: aed69mlz
2022-06-16 20:54:26,883 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:54:26,884 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 256
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 20:54:26,892 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=256 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 20:54:31,906 - wandb.wandb_agent - INFO - Running runs: ['ua507151']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_205432-ua507151
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-72
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ua507151
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64966
wandb:    train_loss 1.17217
wandb: training_time 1.43482
wandb:        val_f1 0.62599
wandb:      val_loss 1.24812
wandb: 
wandb: Synced noble-sweep-72: https://wandb.ai/jah377/sffSHA_arxiv/runs/ua507151
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_205432-ua507151/logs
2022-06-16 21:08:12,155 - wandb.wandb_agent - INFO - Cleaning up finished run: ua507151
2022-06-16 21:08:12,626 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:08:12,626 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 21:08:12,636 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 21:08:17,651 - wandb.wandb_agent - INFO - Running runs: ['9iy3sqtf']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_210817-9iy3sqtf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-73
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/9iy3sqtf
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.59879
wandb:    train_loss 1.3532
wandb: training_time 1.08606
wandb:        val_f1 0.59925
wandb:      val_loss 1.37534
wandb: 
wandb: Synced sandy-sweep-73: https://wandb.ai/jah377/sffSHA_arxiv/runs/9iy3sqtf
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_210817-9iy3sqtf/logs
2022-06-16 21:20:16,923 - wandb.wandb_agent - INFO - Cleaning up finished run: 9iy3sqtf
2022-06-16 21:20:17,901 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:20:17,901 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 21:20:17,909 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 21:20:22,924 - wandb.wandb_agent - INFO - Running runs: ['tjyrbkew']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_212023-tjyrbkew
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-74
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/tjyrbkew
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.52312
wandb:    train_loss 1.68917
wandb: training_time 1.50977
wandb:        val_f1 0.54935
wandb:      val_loss 1.60646
wandb: 
wandb: Synced sweepy-sweep-74: https://wandb.ai/jah377/sffSHA_arxiv/runs/tjyrbkew
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_212023-tjyrbkew/logs
2022-06-16 21:35:02,323 - wandb.wandb_agent - INFO - Cleaning up finished run: tjyrbkew
2022-06-16 21:35:03,233 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:35:03,234 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 256
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 21:35:03,242 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=256 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 21:35:08,256 - wandb.wandb_agent - INFO - Running runs: ['wqni8u3a']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_213508-wqni8u3a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-75
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/wqni8u3a
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.62371
wandb:    train_loss 1.28578
wandb: training_time 1.08925
wandb:        val_f1 0.60881
wandb:      val_loss 1.33963
wandb: 
wandb: Synced balmy-sweep-75: https://wandb.ai/jah377/sffSHA_arxiv/runs/wqni8u3a
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_213508-wqni8u3a/logs
2022-06-16 21:46:19,309 - wandb.wandb_agent - INFO - Cleaning up finished run: wqni8u3a
2022-06-16 21:46:19,839 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:46:19,840 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 21:46:19,848 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 21:46:24,862 - wandb.wandb_agent - INFO - Running runs: ['02fv25zt']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_214625-02fv25zt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-76
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/02fv25zt
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.77573
wandb:    train_loss 0.69107
wandb: training_time 1.13788
wandb:        val_f1 0.63646
wandb:      val_loss 1.25825
wandb: 
wandb: Synced true-sweep-76: https://wandb.ai/jah377/sffSHA_arxiv/runs/02fv25zt
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_214625-02fv25zt/logs
2022-06-16 21:58:29,899 - wandb.wandb_agent - INFO - Cleaning up finished run: 02fv25zt
2022-06-16 21:58:30,464 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:58:30,464 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 21:58:30,471 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 21:58:35,482 - wandb.wandb_agent - INFO - Running runs: ['pk3lhfkh']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_215835-pk3lhfkh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-77
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/pk3lhfkh
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.67447
wandb:    train_loss 1.03562
wandb: training_time 1.15316
wandb:        val_f1 0.63381
wandb:      val_loss 1.21311
wandb: 
wandb: Synced classic-sweep-77: https://wandb.ai/jah377/sffSHA_arxiv/runs/pk3lhfkh
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_215835-pk3lhfkh/logs
2022-06-16 22:10:22,445 - wandb.wandb_agent - INFO - Cleaning up finished run: pk3lhfkh
2022-06-16 22:10:22,921 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:10:22,921 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 22:10:22,929 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 22:10:27,943 - wandb.wandb_agent - INFO - Running runs: ['7p4j7oqo']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_221028-7p4j7oqo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-78
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/7p4j7oqo
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.69809
wandb:    train_loss 0.98965
wandb: training_time 1.0418
wandb:        val_f1 0.62633
wandb:      val_loss 1.23294
wandb: 
wandb: Synced clean-sweep-78: https://wandb.ai/jah377/sffSHA_arxiv/runs/7p4j7oqo
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_221028-7p4j7oqo/logs
2022-06-16 22:22:25,426 - wandb.wandb_agent - INFO - Cleaning up finished run: 7p4j7oqo
2022-06-16 22:22:26,002 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:22:26,002 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 22:22:26,009 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 22:22:31,023 - wandb.wandb_agent - INFO - Running runs: ['lmdsxvex']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_222231-lmdsxvex
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-79
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/lmdsxvex
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.79099
wandb:    train_loss 0.66781
wandb: training_time 1.39585
wandb:        val_f1 0.64495
wandb:      val_loss 1.205
wandb: 
wandb: Synced solar-sweep-79: https://wandb.ai/jah377/sffSHA_arxiv/runs/lmdsxvex
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_222231-lmdsxvex/logs
2022-06-16 22:36:16,243 - wandb.wandb_agent - INFO - Cleaning up finished run: lmdsxvex
2022-06-16 22:36:16,753 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:36:16,755 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 22:36:16,762 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 22:36:21,776 - wandb.wandb_agent - INFO - Running runs: ['wmutjupf']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_223621-wmutjupf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-80
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/wmutjupf
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.74355
wandb:    train_loss 0.8129
wandb: training_time 1.73235
wandb:        val_f1 0.63469
wandb:      val_loss 1.22963
wandb: 
wandb: Synced expert-sweep-80: https://wandb.ai/jah377/sffSHA_arxiv/runs/wmutjupf
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_223621-wmutjupf/logs
2022-06-16 22:51:04,650 - wandb.wandb_agent - INFO - Cleaning up finished run: wmutjupf
2022-06-16 22:51:05,265 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:51:05,266 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 256
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 22:51:05,272 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=256 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 22:51:10,286 - wandb.wandb_agent - INFO - Running runs: ['966ws24s']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_225110-966ws24s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-81
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/966ws24s
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.69019
wandb:    train_loss 0.99296
wandb: training_time 1.48477
wandb:        val_f1 0.63579
wandb:      val_loss 1.19972
wandb: 
wandb: Synced zesty-sweep-81: https://wandb.ai/jah377/sffSHA_arxiv/runs/966ws24s
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_225110-966ws24s/logs
2022-06-16 23:05:16,452 - wandb.wandb_agent - INFO - Cleaning up finished run: 966ws24s
2022-06-16 23:05:17,501 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 23:05:17,501 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 23:05:17,510 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 23:05:22,525 - wandb.wandb_agent - INFO - Running runs: ['ddz0baqk']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_230522-ddz0baqk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-82
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ddz0baqk
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.23662
wandb:    train_loss 90303.59304
wandb: training_time 1.36373
wandb:        val_f1 0.24672
wandb:      val_loss 99227.99246
wandb: 
wandb: Synced likely-sweep-82: https://wandb.ai/jah377/sffSHA_arxiv/runs/ddz0baqk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_230522-ddz0baqk/logs
2022-06-16 23:18:46,335 - wandb.wandb_agent - INFO - Cleaning up finished run: ddz0baqk
2022-06-16 23:18:46,848 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 23:18:46,848 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 23:18:46,856 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-16 23:18:51,870 - wandb.wandb_agent - INFO - Running runs: ['q4s83azd']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_231852-q4s83azd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-83
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/q4s83azd
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.51974
wandb:    train_loss 2.05717
wandb: training_time 1.33377
wandb:        val_f1 0.53998
wandb:      val_loss 1.97281
wandb: 
wandb: Synced graceful-sweep-83: https://wandb.ai/jah377/sffSHA_arxiv/runs/q4s83azd
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_231852-q4s83azd/logs
2022-06-16 23:32:21,413 - wandb.wandb_agent - INFO - Cleaning up finished run: q4s83azd
2022-06-16 23:32:21,862 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 23:32:21,863 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 23:32:21,870 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 23:32:26,885 - wandb.wandb_agent - INFO - Running runs: ['15sa94ho']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_233227-15sa94ho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-84
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/15sa94ho
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.59398
wandb:    train_loss 1.38711
wandb: training_time 1.57777
wandb:        val_f1 0.60247
wandb:      val_loss 1.37326
wandb: 
wandb: Synced revived-sweep-84: https://wandb.ai/jah377/sffSHA_arxiv/runs/15sa94ho
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_233227-15sa94ho/logs
2022-06-16 23:46:58,255 - wandb.wandb_agent - INFO - Cleaning up finished run: 15sa94ho
2022-06-16 23:46:58,717 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 23:46:58,718 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 23:46:58,726 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 23:47:03,740 - wandb.wandb_agent - INFO - Running runs: ['4ttqmy83']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_234703-4ttqmy83
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-85
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/4ttqmy83
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.57604
wandb:    train_loss 1.45259
wandb: training_time 0.97734
wandb:        val_f1 0.59056
wandb:      val_loss 1.40173
wandb: 
wandb: Synced lively-sweep-85: https://wandb.ai/jah377/sffSHA_arxiv/runs/4ttqmy83
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_234703-4ttqmy83/logs
2022-06-16 23:57:53,366 - wandb.wandb_agent - INFO - Cleaning up finished run: 4ttqmy83
2022-06-16 23:57:53,813 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 23:57:53,813 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 23:57:53,820 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 23:57:58,834 - wandb.wandb_agent - INFO - Running runs: ['9vda14ti']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220616_235759-9vda14ti
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-86
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/9vda14ti
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.60753
wandb:    train_loss 1.44176
wandb: training_time 1.26519
wandb:        val_f1 0.60864
wandb:      val_loss 1.42372
wandb: 
wandb: Synced sweepy-sweep-86: https://wandb.ai/jah377/sffSHA_arxiv/runs/9vda14ti
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_235759-9vda14ti/logs
2022-06-17 00:11:12,726 - wandb.wandb_agent - INFO - Cleaning up finished run: 9vda14ti
2022-06-17 00:11:13,562 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 00:11:13,563 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 256
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-17 00:11:13,570 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=256 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-17 00:11:18,585 - wandb.wandb_agent - INFO - Running runs: ['cjk65f1y']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_001118-cjk65f1y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-87
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/cjk65f1y
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.49269
wandb:    train_loss 1.74164
wandb: training_time 1.0397
wandb:        val_f1 0.49122
wandb:      val_loss 1.72166
wandb: 
wandb: Synced brisk-sweep-87: https://wandb.ai/jah377/sffSHA_arxiv/runs/cjk65f1y
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_001118-cjk65f1y/logs
2022-06-17 00:22:31,430 - wandb.wandb_agent - INFO - Cleaning up finished run: cjk65f1y
2022-06-17 00:22:31,933 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 00:22:31,933 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 128
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-17 00:22:31,940 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=128 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-17 00:22:36,955 - wandb.wandb_agent - INFO - Running runs: ['jm6thch9']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_002237-jm6thch9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-88
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/jm6thch9
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k2_dot_product_norm0_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.57763
wandb:    train_loss 1.46238
wandb: training_time 1.55466
wandb:        val_f1 0.56834
wandb:      val_loss 1.47889
wandb: 
wandb: Synced helpful-sweep-88: https://wandb.ai/jah377/sffSHA_arxiv/runs/jm6thch9
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_002237-jm6thch9/logs
2022-06-17 00:40:03,518 - wandb.wandb_agent - INFO - Cleaning up finished run: jm6thch9
2022-06-17 00:40:04,218 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 00:40:04,218 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-17 00:40:04,225 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-17 00:40:09,239 - wandb.wandb_agent - INFO - Running runs: ['tsg9jrx4']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_004009-tsg9jrx4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-89
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/tsg9jrx4
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.66896
wandb:    train_loss 1.05956
wandb: training_time 0.93289
wandb:        val_f1 0.62885
wandb:      val_loss 1.22141
wandb: 
wandb: Synced jolly-sweep-89: https://wandb.ai/jah377/sffSHA_arxiv/runs/tsg9jrx4
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_004009-tsg9jrx4/logs
2022-06-17 00:51:20,156 - wandb.wandb_agent - INFO - Cleaning up finished run: tsg9jrx4
2022-06-17 00:51:20,672 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 00:51:20,673 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 5
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-17 00:51:20,682 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=5 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-17 00:51:25,696 - wandb.wandb_agent - INFO - Running runs: ['hhn2zjdx']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_005125-hhn2zjdx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-90
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/hhn2zjdx
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.5202
wandb:    train_loss 1.61798
wandb: training_time 1.66896
wandb:        val_f1 0.52146
wandb:      val_loss 1.57179
wandb: 
wandb: Synced breezy-sweep-90: https://wandb.ai/jah377/sffSHA_arxiv/runs/hhn2zjdx
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_005125-hhn2zjdx/logs
2022-06-17 01:07:19,286 - wandb.wandb_agent - INFO - Cleaning up finished run: hhn2zjdx
2022-06-17 01:07:19,823 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 01:07:19,824 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 4
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 256
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-17 01:07:19,832 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=4 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=256 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-17 01:07:24,845 - wandb.wandb_agent - INFO - Running runs: ['j6ahe6z6']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_010725-j6ahe6z6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-91
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/j6ahe6z6
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.21394
wandb:    train_loss 3.48133
wandb: training_time 0.96798
wandb:        val_f1 0.23732
wandb:      val_loss 3.49101
wandb: 
wandb: Synced radiant-sweep-91: https://wandb.ai/jah377/sffSHA_arxiv/runs/j6ahe6z6
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_010725-j6ahe6z6/logs
2022-06-17 01:18:14,328 - wandb.wandb_agent - INFO - Cleaning up finished run: j6ahe6z6
2022-06-17 01:18:14,899 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 01:18:14,900 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-17 01:18:14,908 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-17 01:18:19,923 - wandb.wandb_agent - INFO - Running runs: ['tbjq8h7z']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_011820-tbjq8h7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-92
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/tbjq8h7z
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k4_dot_product_norm0_heads1.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.66563
wandb:    train_loss 1.09349
wandb: training_time 1.15509
wandb:        val_f1 0.62076
wandb:      val_loss 1.25496
wandb: 
wandb: Synced fiery-sweep-92: https://wandb.ai/jah377/sffSHA_arxiv/runs/tbjq8h7z
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_011820-tbjq8h7z/logs
2022-06-17 01:34:13,562 - wandb.wandb_agent - INFO - Cleaning up finished run: tbjq8h7z
2022-06-17 01:34:14,249 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 01:34:14,250 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-17 01:34:14,256 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-17 01:34:19,270 - wandb.wandb_agent - INFO - Running runs: ['ftfjz0h7']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_013419-ftfjz0h7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-93
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/ftfjz0h7
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.85119
wandb:    train_loss 0.47365
wandb: training_time 1.67522
wandb:        val_f1 0.62425
wandb:      val_loss 1.38049
wandb: 
wandb: Synced clean-sweep-93: https://wandb.ai/jah377/sffSHA_arxiv/runs/ftfjz0h7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_013419-ftfjz0h7/logs
2022-06-17 01:49:42,981 - wandb.wandb_agent - INFO - Cleaning up finished run: ftfjz0h7
2022-06-17 01:49:43,954 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 01:49:43,954 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 128
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-17 01:49:43,962 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=128 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-17 01:49:48,978 - wandb.wandb_agent - INFO - Running runs: ['qhq2mpi6']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_014949-qhq2mpi6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-94
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/qhq2mpi6
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53921
wandb:    train_loss 1.63719
wandb: training_time 0.74241
wandb:        val_f1 0.54324
wandb:      val_loss 1.61876
wandb: 
wandb: Synced sleek-sweep-94: https://wandb.ai/jah377/sffSHA_arxiv/runs/qhq2mpi6
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_014949-qhq2mpi6/logs
2022-06-17 01:58:34,596 - wandb.wandb_agent - INFO - Cleaning up finished run: qhq2mpi6
2022-06-17 01:58:35,765 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 01:58:35,765 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-17 01:58:35,772 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
2022-06-17 01:58:40,786 - wandb.wandb_agent - INFO - Running runs: ['vf4vxofu']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_015841-vf4vxofu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-95
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/vf4vxofu
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.22834
wandb:    train_loss 3.48679
wandb: training_time 1.16894
wandb:        val_f1 0.27491
wandb:      val_loss 3.4629
wandb: 
wandb: Synced sweepy-sweep-95: https://wandb.ai/jah377/sffSHA_arxiv/runs/vf4vxofu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_015841-vf4vxofu/logs
2022-06-17 02:11:09,073 - wandb.wandb_agent - INFO - Cleaning up finished run: vf4vxofu
2022-06-17 02:11:09,590 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 02:11:09,591 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-17 02:11:09,600 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-17 02:11:14,614 - wandb.wandb_agent - INFO - Running runs: ['2jbfy5z3']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_021114-2jbfy5z3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-96
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/2jbfy5z3
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.78269
wandb:    train_loss 0.71833
wandb: training_time 1.62174
wandb:        val_f1 0.63962
wandb:      val_loss 1.23902
wandb: 
wandb: Synced lyric-sweep-96: https://wandb.ai/jah377/sffSHA_arxiv/runs/2jbfy5z3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_021114-2jbfy5z3/logs
2022-06-17 02:26:06,593 - wandb.wandb_agent - INFO - Cleaning up finished run: 2jbfy5z3
2022-06-17 02:26:07,085 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 02:26:07,085 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 128
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-17 02:26:07,093 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=128 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-17 02:26:12,106 - wandb.wandb_agent - INFO - Running runs: ['eem55wy3']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_022612-eem55wy3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-97
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/eem55wy3
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.2312
wandb:    train_loss 3.37537
wandb: training_time 1.26452
wandb:        val_f1 0.27145
wandb:      val_loss 3.33397
wandb: 
wandb: Synced giddy-sweep-97: https://wandb.ai/jah377/sffSHA_arxiv/runs/eem55wy3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_022612-eem55wy3/logs
2022-06-17 02:38:09,087 - wandb.wandb_agent - INFO - Cleaning up finished run: eem55wy3
2022-06-17 02:38:10,308 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 02:38:10,309 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 256
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1
2022-06-17 02:38:10,315 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=256 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1
Using backend: pytorch
2022-06-17 02:38:15,326 - wandb.wandb_agent - INFO - Running runs: ['czp3f1zb']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_023815-czp3f1zb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-98
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/czp3f1zb
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.31401
wandb:    train_loss 2.69129
wandb: training_time 0.95874
wandb:        val_f1 0.32538
wandb:      val_loss 2.63665
wandb: 
wandb: Synced crisp-sweep-98: https://wandb.ai/jah377/sffSHA_arxiv/runs/czp3f1zb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_023815-czp3f1zb/logs
2022-06-17 02:48:52,442 - wandb.wandb_agent - INFO - Cleaning up finished run: czp3f1zb
2022-06-17 02:48:53,070 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 02:48:53,071 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-17 02:48:53,079 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-17 02:48:58,093 - wandb.wandb_agent - INFO - Running runs: ['1eoqdfkv']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_024858-1eoqdfkv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-99
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/1eoqdfkv
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.59295
wandb:    train_loss 1.53671
wandb: training_time 1.37633
wandb:        val_f1 0.60123
wandb:      val_loss 1.50252
wandb: 
wandb: Synced celestial-sweep-99: https://wandb.ai/jah377/sffSHA_arxiv/runs/1eoqdfkv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_024858-1eoqdfkv/logs
2022-06-17 03:02:22,594 - wandb.wandb_agent - INFO - Cleaning up finished run: 1eoqdfkv
2022-06-17 03:02:23,112 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-17 03:02:23,112 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 1
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 128
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-17 03:02:23,119 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=1 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=128 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-17 03:02:28,134 - wandb.wandb_agent - INFO - Running runs: ['2hy5sblb']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffSHA_arxiv/wandb/run-20220617_030228-2hy5sblb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-100
wandb:  View project at https://wandb.ai/jah377/sffSHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb:  View run at https://wandb.ai/jah377/sffSHA_arxiv/runs/2hy5sblb
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 1, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.61199
wandb:    train_loss 1.26302
wandb: training_time 0.95743
wandb:        val_f1 0.55988
wandb:      val_loss 1.49132
wandb: 
wandb: Synced smart-sweep-100: https://wandb.ai/jah377/sffSHA_arxiv/runs/2hy5sblb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220617_030228-2hy5sblb/logs
2022-06-17 03:12:47,525 - wandb.wandb_agent - INFO - Cleaning up finished run: 2hy5sblb
wandb: Terminating and syncing runs. Press ctrl-c to kill.
Create sweep with ID: kdefof12
Sweep URL: https://wandb.ai/jah377/sffSHA_arxiv/sweeps/kdefof12
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: - 0.119 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: \ 0.267 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: | 0.267 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: / 0.267 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: - 0.267 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: \ 0.267 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: | 0.267 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: / 0.267 MB of 0.267 MB uploaded (0.000 MB deduped)wandb: - 0.267 MB of 0.267 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced jumping-planet-65: https://wandb.ai/jah377/sffSHA_arxiv/runs/253miwx5
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_043510-253miwx5/logs
==================================================

++++++++++++++++++++++++++++++++++++++++++++++++++++
TOTAL RUNTIME = 22 hours 38 minutes
++++++++++++++++++++++++++++++++++++++++++++++++++++
