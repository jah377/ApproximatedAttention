Thu 16 Jun 2022 03:45:50 AM CEST
r32n1.lisa.surfsara.nl
uid=55639(jharris) gid=55199(jharris) groups=55199(jharris),46457(lisa_uva_gpu),50488(ssh_forwarding)
/home/jharris/Desktop/approx_attention
/home/jharris/Desktop/approx_attention/venvs/GPU_venv/bin/python
/home/jharris/Desktop/approx_attention/venvs/GPU_venv/bin/python

Check if packages installed correctly
Torch: 1.11.0+cu113
PyG: 2.0.4


==================================================
dataset: arxiv
method: bayes
model: SIGNff_MHA
iterations: 100
run_trial: false
config: SIGNff_MHA.yaml
train_file: hps_SIGNff_DPA.py
project_name: sffMHA_arxiv
method: bayes
metric:
  goal: minimize
  name: val_loss
parameters:
  ATTN_HEADS:
    distribution: int_uniform
    max: 5
    min: 2
  BATCH_NORMALIZATION:
    value: 1
  BATCH_SIZE:
    values:
    - 2048
    - 4096
    - 8192
    - 16384
  CLASSIFICATION_LAYERS:
    distribution: int_uniform
    max: 3
    min: 1
  CLASSIFICATION_UNITS:
    values:
    - 128
    - 256
    - 512
    - 1024
  DATASET:
    value: arxiv
  DPA_NORMALIZATION:
    values:
    - 0
    - 1
  EPOCHS:
    value: 300
  FEATURE_DROPOUT:
    values:
    - 0.0
    - 0.1
    - 0.2
    - 0.3
    - 0.4
    - 0.5
    - 0.6
    - 0.7
  HOPS:
    values:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
  INCEPTION_LAYERS:
    distribution: int_uniform
    max: 3
    min: 1
  INCEPTION_UNITS:
    values:
    - 128
    - 256
    - 512
    - 1024
  LEARNING_RATE:
    values:
    - 1.0
    - 0.1
    - 0.01
    - 0.001
    - 0.0001
    - 1.0e-05
    - 1.0e-06
    - 1.0e-07
  LR_PATIENCE:
    value: 5
  NODE_DROPOUT:
    values:
    - 0.0
    - 0.1
    - 0.2
    - 0.3
    - 0.4
    - 0.5
    - 0.6
    - 0.7
  SEED:
    value: 42
  TERMINATION_PATIENCE:
    value: 10
  TRANSFORMATION:
    value: dot_product
  WEIGHT_DECAY:
    values:
    - 1.0
    - 0.1
    - 0.01
    - 0.001
    - 0.0001
    - 1.0e-05
    - 1.0e-06
    - 1.0e-07
program: hps_SIGNff_DPA.py

wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_034641-2o824h0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-pond-99
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/2o824h0n
wandb: Starting wandb agent \U0001f575\ufe0f
2022-06-16 03:46:49,508 - wandb.wandb_agent - INFO - Running runs: []
2022-06-16 03:46:49,769 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 03:46:49,769 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 2
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 128
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 03:46:49,777 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=2 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=128 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 03:46:54,786 - wandb.wandb_agent - INFO - Running runs: ['09obrb0x']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_034653-09obrb0x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-1
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/09obrb0x
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 2, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k4_dot_product_norm1_heads2.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.25933
wandb:    train_loss 2.9349
wandb: training_time 1.26212
wandb:        val_f1 0.2934
wandb:      val_loss 2.84284
wandb: 
wandb: Synced comfy-sweep-1: https://wandb.ai/jah377/sffMHA_arxiv/runs/09obrb0x
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_034653-09obrb0x/logs
2022-06-16 04:03:47,498 - wandb.wandb_agent - INFO - Cleaning up finished run: 09obrb0x
2022-06-16 04:03:47,883 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 04:03:47,884 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 04:03:47,892 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 04:03:52,904 - wandb.wandb_agent - INFO - Running runs: ['ymdn4zo2']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_040352-ymdn4zo2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-2
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/ymdn4zo2
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k1_dot_product_norm0_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.6256
wandb:    train_loss 1.26244
wandb: training_time 0.72249
wandb:        val_f1 0.61284
wandb:      val_loss 1.3106
wandb: 
wandb: Synced ethereal-sweep-2: https://wandb.ai/jah377/sffMHA_arxiv/runs/ymdn4zo2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_040352-ymdn4zo2/logs
2022-06-16 04:17:22,850 - wandb.wandb_agent - INFO - Cleaning up finished run: ymdn4zo2
2022-06-16 04:17:23,244 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 04:17:23,245 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 5
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 128
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 04:17:23,252 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=5 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=128 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 04:17:28,265 - wandb.wandb_agent - INFO - Running runs: ['pbgtdzu4']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_041728-pbgtdzu4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-3
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/pbgtdzu4
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 128, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k5_dot_product_norm0_heads4.pth

$$$ EARLY STOPPING TRIGGERED $$$
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 15
wandb:      train_f1 0.02816
wandb:    train_loss 3.70097
wandb: training_time 0.84932
wandb:        val_f1 0.02366
wandb:      val_loss 3.71518
wandb: 
wandb: Synced treasured-sweep-3: https://wandb.ai/jah377/sffMHA_arxiv/runs/pbgtdzu4
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_041728-pbgtdzu4/logs
2022-06-16 04:22:42,717 - wandb.wandb_agent - INFO - Cleaning up finished run: pbgtdzu4
2022-06-16 04:22:43,145 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 04:22:43,145 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 3
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 5
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 256
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.7
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 04:22:43,152 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=3 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=5 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=256 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.7 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 04:22:48,165 - wandb.wandb_agent - INFO - Running runs: ['6ysjqzr7']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_042248-6ysjqzr7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-4
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/6ysjqzr7
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.7, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 3, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k5_dot_product_norm1_heads3.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.17924
wandb:    train_loss 3.80342
wandb: training_time 1.36674
wandb:        val_f1 0.07628
wandb:      val_loss 3.48539
wandb: 
wandb: Synced crimson-sweep-4: https://wandb.ai/jah377/sffMHA_arxiv/runs/6ysjqzr7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_042248-6ysjqzr7/logs
2022-06-16 04:40:17,846 - wandb.wandb_agent - INFO - Cleaning up finished run: 6ysjqzr7
2022-06-16 04:40:18,245 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 04:40:18,246 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 2
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 04:40:18,253 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=2 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-16 04:40:23,262 - wandb.wandb_agent - INFO - Running runs: ['1mjd012p']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_044023-1mjd012p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-5
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/1mjd012p
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 2, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm1_heads2.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.5723
wandb:    train_loss 1.59391
wandb: training_time 0.96145
wandb:        val_f1 0.55562
wandb:      val_loss 1.63606
wandb: 
wandb: Synced laced-sweep-5: https://wandb.ai/jah377/sffMHA_arxiv/runs/1mjd012p
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_044023-1mjd012p/logs
2022-06-16 04:55:36,706 - wandb.wandb_agent - INFO - Cleaning up finished run: 1mjd012p
2022-06-16 04:55:37,095 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 04:55:37,095 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 04:55:37,103 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 04:55:42,116 - wandb.wandb_agent - INFO - Running runs: ['2mjblaoc']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_045542-2mjblaoc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-6
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/2mjblaoc
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k1_dot_product_norm0_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.70789
wandb:    train_loss 0.92699
wandb: training_time 1.06865
wandb:        val_f1 0.62274
wandb:      val_loss 1.25014
wandb: 
wandb: Synced restful-sweep-6: https://wandb.ai/jah377/sffMHA_arxiv/runs/2mjblaoc
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_045542-2mjblaoc/logs
2022-06-16 05:11:30,083 - wandb.wandb_agent - INFO - Cleaning up finished run: 2mjblaoc
2022-06-16 05:11:30,482 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 05:11:30,483 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 1
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1
2022-06-16 05:11:30,490 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=1 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 05:11:35,502 - wandb.wandb_agent - INFO - Running runs: ['sh35vktu']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_051135-sh35vktu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-7
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/sh35vktu
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 1, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k1_dot_product_norm1_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.06985
wandb:    train_loss 3.66326
wandb: training_time 1.05204
wandb:        val_f1 0.09138
wandb:      val_loss 3.64684
wandb: 
wandb: Synced rose-sweep-7: https://wandb.ai/jah377/sffMHA_arxiv/runs/sh35vktu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_051135-sh35vktu/logs
2022-06-16 05:27:02,529 - wandb.wandb_agent - INFO - Cleaning up finished run: sh35vktu
2022-06-16 05:27:03,000 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 05:27:03,001 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 2
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 256
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 05:27:03,008 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=2 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=256 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 05:27:08,021 - wandb.wandb_agent - INFO - Running runs: ['2vg7jyco']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_052708-2vg7jyco
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-8
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/2vg7jyco
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 2, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k1_dot_product_norm1_heads2.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.5828
wandb:    train_loss 1.41188
wandb: training_time 0.81148
wandb:        val_f1 0.58099
wandb:      val_loss 1.40638
wandb: 
wandb: Synced twilight-sweep-8: https://wandb.ai/jah377/sffMHA_arxiv/runs/2vg7jyco
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_052708-2vg7jyco/logs
2022-06-16 05:41:03,105 - wandb.wandb_agent - INFO - Cleaning up finished run: 2vg7jyco
2022-06-16 05:41:03,503 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 05:41:03,503 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 05:41:03,511 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 05:41:08,522 - wandb.wandb_agent - INFO - Running runs: ['saoyqm87']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_054108-saoyqm87
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-9
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/saoyqm87
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k2_dot_product_norm0_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.63117
wandb:    train_loss 1.22356
wandb: training_time 1.00261
wandb:        val_f1 0.60069
wandb:      val_loss 1.3326
wandb: 
wandb: Synced whole-sweep-9: https://wandb.ai/jah377/sffMHA_arxiv/runs/saoyqm87
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_054108-saoyqm87/logs
2022-06-16 05:56:31,410 - wandb.wandb_agent - INFO - Cleaning up finished run: saoyqm87
2022-06-16 05:56:31,845 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 05:56:31,846 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 05:56:31,854 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 05:56:36,866 - wandb.wandb_agent - INFO - Running runs: ['pvhj9t7p']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_055636-pvhj9t7p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-10
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/pvhj9t7p
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm1_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.55712
wandb:    train_loss 1.51308
wandb: training_time 0.8593
wandb:        val_f1 0.53193
wandb:      val_loss 1.60064
wandb: 
wandb: Synced radiant-sweep-10: https://wandb.ai/jah377/sffMHA_arxiv/runs/pvhj9t7p
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_055636-pvhj9t7p/logs
2022-06-16 06:10:22,288 - wandb.wandb_agent - INFO - Cleaning up finished run: pvhj9t7p
2022-06-16 06:10:22,692 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:10:22,693 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 06:10:22,700 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 06:10:27,714 - wandb.wandb_agent - INFO - Running runs: ['2btn6cuy']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_061027-2btn6cuy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-11
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/2btn6cuy
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.63362
wandb:    train_loss 1.19272
wandb: training_time 0.96503
wandb:        val_f1 0.60656
wandb:      val_loss 1.31005
wandb: 
wandb: Synced ruby-sweep-11: https://wandb.ai/jah377/sffMHA_arxiv/runs/2btn6cuy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_061027-2btn6cuy/logs
2022-06-16 06:21:07,083 - wandb.wandb_agent - INFO - Cleaning up finished run: 2btn6cuy
2022-06-16 06:21:07,624 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:21:07,624 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 3
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 06:21:07,633 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=3 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 06:21:12,646 - wandb.wandb_agent - INFO - Running runs: ['5ud0u8a0']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_062112-5ud0u8a0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-12
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/5ud0u8a0
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 3, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm0_heads3.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.46304
wandb:    train_loss 1.90609
wandb: training_time 0.82224
wandb:        val_f1 0.45985
wandb:      val_loss 1.90755
wandb: 
wandb: Synced expert-sweep-12: https://wandb.ai/jah377/sffMHA_arxiv/runs/5ud0u8a0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_062112-5ud0u8a0/logs
2022-06-16 06:34:57,652 - wandb.wandb_agent - INFO - Cleaning up finished run: 5ud0u8a0
2022-06-16 06:34:58,070 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:34:58,070 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 06:34:58,079 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 06:35:03,090 - wandb.wandb_agent - INFO - Running runs: ['ifzqcno8']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_063503-ifzqcno8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-13
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/ifzqcno8
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm0_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.50203
wandb:    train_loss 1.77796
wandb: training_time 0.78162
wandb:        val_f1 0.50636
wandb:      val_loss 1.77059
wandb: 
wandb: Synced balmy-sweep-13: https://wandb.ai/jah377/sffMHA_arxiv/runs/ifzqcno8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_063503-ifzqcno8/logs
2022-06-16 06:48:37,221 - wandb.wandb_agent - INFO - Cleaning up finished run: ifzqcno8
2022-06-16 06:48:37,649 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:48:37,649 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 06:48:37,659 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
2022-06-16 06:48:42,672 - wandb.wandb_agent - INFO - Running runs: ['7sjbfu3x']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_064842-7sjbfu3x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-14
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/7sjbfu3x
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.60433
wandb:    train_loss 1.34236
wandb: training_time 0.87251
wandb:        val_f1 0.58683
wandb:      val_loss 1.39501
wandb: 
wandb: Synced faithful-sweep-14: https://wandb.ai/jah377/sffMHA_arxiv/runs/7sjbfu3x
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_064842-7sjbfu3x/logs
2022-06-16 06:58:49,381 - wandb.wandb_agent - INFO - Cleaning up finished run: 7sjbfu3x
2022-06-16 06:58:49,847 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 06:58:49,847 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 06:58:49,855 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 06:58:54,868 - wandb.wandb_agent - INFO - Running runs: ['ewcpwyjw']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_065854-ewcpwyjw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-15
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/ewcpwyjw
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k2_dot_product_norm0_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.66003
wandb:    train_loss 1.15277
wandb: training_time 1.00525
wandb:        val_f1 0.60999
wandb:      val_loss 1.32325
wandb: 
wandb: Synced zesty-sweep-15: https://wandb.ai/jah377/sffMHA_arxiv/runs/ewcpwyjw
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_065854-ewcpwyjw/logs
2022-06-16 07:14:18,565 - wandb.wandb_agent - INFO - Cleaning up finished run: ewcpwyjw
2022-06-16 07:14:19,046 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:14:19,046 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 1
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.6
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 07:14:19,055 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=1 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.6 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 07:14:24,069 - wandb.wandb_agent - INFO - Running runs: ['1ymvyo8n']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_071424-1ymvyo8n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-16
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/1ymvyo8n
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 1, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.6, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.32731
wandb:    train_loss 3.04471
wandb: training_time 0.69777
wandb:        val_f1 0.33286
wandb:      val_loss 3.12037
wandb: 
wandb: Synced driven-sweep-16: https://wandb.ai/jah377/sffMHA_arxiv/runs/1ymvyo8n
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_071424-1ymvyo8n/logs
2022-06-16 07:22:54,011 - wandb.wandb_agent - INFO - Cleaning up finished run: 1ymvyo8n
2022-06-16 07:22:54,452 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:22:54,452 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 07:22:54,460 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 07:22:59,474 - wandb.wandb_agent - INFO - Running runs: ['36rfmmhd']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_072259-36rfmmhd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-17
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/36rfmmhd
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.66934
wandb:    train_loss 1.06227
wandb: training_time 0.74938
wandb:        val_f1 0.57542
wandb:      val_loss 1.43826
wandb: 
wandb: Synced soft-sweep-17: https://wandb.ai/jah377/sffMHA_arxiv/runs/36rfmmhd
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_072259-36rfmmhd/logs
2022-06-16 07:31:46,796 - wandb.wandb_agent - INFO - Cleaning up finished run: 36rfmmhd
2022-06-16 07:31:47,213 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:31:47,213 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 3
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 512
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 07:31:47,222 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=3 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=512 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 07:31:52,234 - wandb.wandb_agent - INFO - Running runs: ['5d8v2g09']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_073152-5d8v2g09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-18
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/5d8v2g09
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k3_dot_product_norm0_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.17896
wandb:    train_loss 3.27447
wandb: training_time 1.3839
wandb:        val_f1 0.07624
wandb:      val_loss 3.14533
wandb: 
wandb: Synced feasible-sweep-18: https://wandb.ai/jah377/sffMHA_arxiv/runs/5d8v2g09
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_073152-5d8v2g09/logs
2022-06-16 07:49:39,063 - wandb.wandb_agent - INFO - Cleaning up finished run: 5d8v2g09
2022-06-16 07:49:39,560 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:49:39,561 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 07:49:39,568 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 07:49:44,583 - wandb.wandb_agent - INFO - Running runs: ['stz1ycj7']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_074944-stz1ycj7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-19
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/stz1ycj7
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53896
wandb:    train_loss 1.60692
wandb: training_time 0.85143
wandb:        val_f1 0.54193
wandb:      val_loss 1.5646
wandb: 
wandb: Synced playful-sweep-19: https://wandb.ai/jah377/sffMHA_arxiv/runs/stz1ycj7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_074944-stz1ycj7/logs
2022-06-16 07:59:35,760 - wandb.wandb_agent - INFO - Cleaning up finished run: stz1ycj7
2022-06-16 07:59:36,205 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 07:59:36,206 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 07:59:36,213 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 07:59:41,226 - wandb.wandb_agent - INFO - Running runs: ['vvphyhoe']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_075941-vvphyhoe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-20
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/vvphyhoe
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm1_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.47961
wandb:    train_loss 1.81506
wandb: training_time 0.94613
wandb:        val_f1 0.48055
wandb:      val_loss 1.79339
wandb: 
wandb: Synced eager-sweep-20: https://wandb.ai/jah377/sffMHA_arxiv/runs/vvphyhoe
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_075941-vvphyhoe/logs
2022-06-16 08:13:25,643 - wandb.wandb_agent - INFO - Cleaning up finished run: vvphyhoe
2022-06-16 08:13:26,045 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 08:13:26,045 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 08:13:26,054 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 08:13:31,067 - wandb.wandb_agent - INFO - Running runs: ['fm4slk4h']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_081331-fm4slk4h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-21
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/fm4slk4h
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.50762
wandb:    train_loss 1.72696
wandb: training_time 0.72334
wandb:        val_f1 0.51401
wandb:      val_loss 1.6852
wandb: 
wandb: Synced happy-sweep-21: https://wandb.ai/jah377/sffMHA_arxiv/runs/fm4slk4h
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_081331-fm4slk4h/logs
2022-06-16 08:22:11,790 - wandb.wandb_agent - INFO - Cleaning up finished run: fm4slk4h
2022-06-16 08:22:12,273 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 08:22:12,274 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 08:22:12,280 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 08:22:17,294 - wandb.wandb_agent - INFO - Running runs: ['a6u14jet']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_082217-a6u14jet
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-22
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/a6u14jet
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm0_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.72094
wandb:    train_loss 0.89122
wandb: training_time 0.87674
wandb:        val_f1 0.56512
wandb:      val_loss 1.49024
wandb: 
wandb: Synced winter-sweep-22: https://wandb.ai/jah377/sffMHA_arxiv/runs/a6u14jet
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_082217-a6u14jet/logs
2022-06-16 08:36:33,666 - wandb.wandb_agent - INFO - Cleaning up finished run: a6u14jet
2022-06-16 08:36:34,071 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 08:36:34,071 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 08:36:34,081 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 08:36:39,094 - wandb.wandb_agent - INFO - Running runs: ['thxb8b5k']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_083639-thxb8b5k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-23
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/thxb8b5k
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k1_dot_product_norm1_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65947
wandb:    train_loss 1.09133
wandb: training_time 1.03465
wandb:        val_f1 0.62039
wandb:      val_loss 1.25088
wandb: 
wandb: Synced sandy-sweep-23: https://wandb.ai/jah377/sffMHA_arxiv/runs/thxb8b5k
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_083639-thxb8b5k/logs
2022-06-16 08:52:06,496 - wandb.wandb_agent - INFO - Cleaning up finished run: thxb8b5k
2022-06-16 08:52:06,903 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 08:52:06,904 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 08:52:06,913 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 08:52:11,926 - wandb.wandb_agent - INFO - Running runs: ['0ijmwv3e']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_085211-0ijmwv3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-24
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/0ijmwv3e
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.6357
wandb:    train_loss 1.21118
wandb: training_time 0.92526
wandb:        val_f1 0.61713
wandb:      val_loss 1.27267
wandb: 
wandb: Synced whole-sweep-24: https://wandb.ai/jah377/sffMHA_arxiv/runs/0ijmwv3e
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_085211-0ijmwv3e/logs
2022-06-16 09:02:09,368 - wandb.wandb_agent - INFO - Cleaning up finished run: 0ijmwv3e
2022-06-16 09:02:09,756 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:02:09,756 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 09:02:09,765 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 09:02:14,779 - wandb.wandb_agent - INFO - Running runs: ['g2hy3ih8']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_090214-g2hy3ih8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-25
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/g2hy3ih8
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64913
wandb:    train_loss 1.15618
wandb: training_time 0.73649
wandb:        val_f1 0.58509
wandb:      val_loss 1.41726
wandb: 
wandb: Synced twilight-sweep-25: https://wandb.ai/jah377/sffMHA_arxiv/runs/g2hy3ih8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_090214-g2hy3ih8/logs
2022-06-16 09:11:05,917 - wandb.wandb_agent - INFO - Cleaning up finished run: g2hy3ih8
2022-06-16 09:11:06,329 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:11:06,330 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 09:11:06,339 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 09:11:11,350 - wandb.wandb_agent - INFO - Running runs: ['c68gjd3u']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_091111-c68gjd3u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-26
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/c68gjd3u
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k2_dot_product_norm1_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.6414
wandb:    train_loss 1.1984
wandb: training_time 1.05143
wandb:        val_f1 0.61885
wandb:      val_loss 1.27475
wandb: 
wandb: Synced frosty-sweep-26: https://wandb.ai/jah377/sffMHA_arxiv/runs/c68gjd3u
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_091111-c68gjd3u/logs
2022-06-16 09:26:27,058 - wandb.wandb_agent - INFO - Cleaning up finished run: c68gjd3u
2022-06-16 09:26:27,490 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:26:27,490 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 09:26:27,497 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-16 09:26:32,510 - wandb.wandb_agent - INFO - Running runs: ['1zsb9ki0']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_092632-1zsb9ki0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-27
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/1zsb9ki0
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.52081
wandb:    train_loss 1.75023
wandb: training_time 0.84021
wandb:        val_f1 0.53445
wandb:      val_loss 1.70807
wandb: 
wandb: Synced astral-sweep-27: https://wandb.ai/jah377/sffMHA_arxiv/runs/1zsb9ki0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_092632-1zsb9ki0/logs
2022-06-16 09:36:26,276 - wandb.wandb_agent - INFO - Cleaning up finished run: 1zsb9ki0
2022-06-16 09:36:26,877 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:36:26,878 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 09:36:26,885 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 09:36:31,898 - wandb.wandb_agent - INFO - Running runs: ['wo4jh7sy']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_093631-wo4jh7sy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-28
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/wo4jh7sy
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.54355
wandb:    train_loss 1.58154
wandb: training_time 0.6883
wandb:        val_f1 0.53683
wandb:      val_loss 1.59641
wandb: 
wandb: Synced magic-sweep-28: https://wandb.ai/jah377/sffMHA_arxiv/runs/wo4jh7sy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_093631-wo4jh7sy/logs
2022-06-16 09:45:11,309 - wandb.wandb_agent - INFO - Cleaning up finished run: wo4jh7sy
2022-06-16 09:45:11,797 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:45:11,797 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 09:45:11,806 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-16 09:45:16,819 - wandb.wandb_agent - INFO - Running runs: ['4nz6z5kb']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_094516-4nz6z5kb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-29
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/4nz6z5kb
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65193
wandb:    train_loss 1.20651
wandb: training_time 0.77953
wandb:        val_f1 0.57968
wandb:      val_loss 1.44944
wandb: 
wandb: Synced kind-sweep-29: https://wandb.ai/jah377/sffMHA_arxiv/runs/4nz6z5kb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_094516-4nz6z5kb/logs
2022-06-16 09:53:59,575 - wandb.wandb_agent - INFO - Cleaning up finished run: 4nz6z5kb
2022-06-16 09:54:00,027 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 09:54:00,028 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 09:54:00,035 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 09:54:05,046 - wandb.wandb_agent - INFO - Running runs: ['9seixtgg']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_095405-9seixtgg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-30
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/9seixtgg
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.47519
wandb:    train_loss 1.8244
wandb: training_time 0.80022
wandb:        val_f1 0.48532
wandb:      val_loss 1.7695
wandb: 
wandb: Synced hearty-sweep-30: https://wandb.ai/jah377/sffMHA_arxiv/runs/9seixtgg
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_095405-9seixtgg/logs
2022-06-16 10:03:25,619 - wandb.wandb_agent - INFO - Cleaning up finished run: 9seixtgg
2022-06-16 10:03:26,023 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:03:26,023 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 3
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 10:03:26,030 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=3 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 10:03:31,042 - wandb.wandb_agent - INFO - Running runs: ['x3gtl7p5']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_100331-x3gtl7p5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-31
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/x3gtl7p5
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 3, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k0_dot_product_norm1_heads3.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.34339
wandb:    train_loss 2.83217
wandb: training_time 0.75811
wandb:        val_f1 0.35964
wandb:      val_loss 4.74758
wandb: 
wandb: Synced fallen-sweep-31: https://wandb.ai/jah377/sffMHA_arxiv/runs/x3gtl7p5
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_100331-x3gtl7p5/logs
2022-06-16 10:17:31,390 - wandb.wandb_agent - INFO - Cleaning up finished run: x3gtl7p5
2022-06-16 10:17:31,912 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:17:31,913 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 10:17:31,920 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 10:17:36,930 - wandb.wandb_agent - INFO - Running runs: ['1wczevst']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_101736-1wczevst
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-32
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/1wczevst
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.62249
wandb:    train_loss 1.27038
wandb: training_time 1.06179
wandb:        val_f1 0.60294
wandb:      val_loss 1.35941
wandb: 
wandb: Synced vivid-sweep-32: https://wandb.ai/jah377/sffMHA_arxiv/runs/1wczevst
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_101736-1wczevst/logs
2022-06-16 10:29:02,599 - wandb.wandb_agent - INFO - Cleaning up finished run: 1wczevst
2022-06-16 10:29:03,175 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:29:03,175 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1
2022-06-16 10:29:03,184 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1
Using backend: pytorch
2022-06-16 10:29:08,199 - wandb.wandb_agent - INFO - Running runs: ['n6g6sio7']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_102908-n6g6sio7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-33
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/n6g6sio7
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.43798
wandb:    train_loss 2.10099
wandb: training_time 1.1211
wandb:        val_f1 0.44988
wandb:      val_loss 2.04286
wandb: 
wandb: Synced avid-sweep-33: https://wandb.ai/jah377/sffMHA_arxiv/runs/n6g6sio7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_102908-n6g6sio7/logs
2022-06-16 10:40:45,636 - wandb.wandb_agent - INFO - Cleaning up finished run: n6g6sio7
2022-06-16 10:40:46,145 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:40:46,145 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 10:40:46,153 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
2022-06-16 10:40:51,168 - wandb.wandb_agent - INFO - Running runs: ['ae2u0oaa']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_104051-ae2u0oaa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-34
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/ae2u0oaa
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k3_dot_product_norm1_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.56241
wandb:    train_loss 1.59849
wandb: training_time 1.12665
wandb:        val_f1 0.57103
wandb:      val_loss 1.55088
wandb: 
wandb: Synced hearty-sweep-34: https://wandb.ai/jah377/sffMHA_arxiv/runs/ae2u0oaa
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_104051-ae2u0oaa/logs
2022-06-16 10:57:16,069 - wandb.wandb_agent - INFO - Cleaning up finished run: ae2u0oaa
2022-06-16 10:57:16,721 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 10:57:16,721 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 10:57:16,728 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 10:57:21,742 - wandb.wandb_agent - INFO - Running runs: ['ayryqolu']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_105721-ayryqolu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-35
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/ayryqolu
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k2_dot_product_norm1_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.60311
wandb:    train_loss 1.37107
wandb: training_time 0.97648
wandb:        val_f1 0.59207
wandb:      val_loss 1.39894
wandb: 
wandb: Synced hearty-sweep-35: https://wandb.ai/jah377/sffMHA_arxiv/runs/ayryqolu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_105721-ayryqolu/logs
2022-06-16 11:12:39,762 - wandb.wandb_agent - INFO - Cleaning up finished run: ayryqolu
2022-06-16 11:12:40,316 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 11:12:40,316 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 11:12:40,325 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 11:12:45,338 - wandb.wandb_agent - INFO - Running runs: ['248bwn3h']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_111245-248bwn3h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-36
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/248bwn3h
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64355
wandb:    train_loss 1.15969
wandb: training_time 0.87987
wandb:        val_f1 0.61992
wandb:      val_loss 1.24909
wandb: 
wandb: Synced olive-sweep-36: https://wandb.ai/jah377/sffMHA_arxiv/runs/248bwn3h
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_111245-248bwn3h/logs
2022-06-16 11:23:04,506 - wandb.wandb_agent - INFO - Cleaning up finished run: 248bwn3h
2022-06-16 11:23:05,554 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 11:23:05,554 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 11:23:05,562 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
2022-06-16 11:23:10,575 - wandb.wandb_agent - INFO - Running runs: ['5j8kw9j3']
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_112312-5j8kw9j3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-37
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/5j8kw9j3
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.56213
wandb:    train_loss 1.49016
wandb: training_time 0.97118
wandb:        val_f1 0.57395
wandb:      val_loss 1.43988
wandb: 
wandb: Synced iconic-sweep-37: https://wandb.ai/jah377/sffMHA_arxiv/runs/5j8kw9j3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_112312-5j8kw9j3/logs
2022-06-16 11:33:24,252 - wandb.wandb_agent - INFO - Cleaning up finished run: 5j8kw9j3
2022-06-16 11:33:24,738 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 11:33:24,739 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 5
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-16 11:33:24,748 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=5 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
2022-06-16 11:33:29,761 - wandb.wandb_agent - INFO - Running runs: ['d2wl6c9f']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_113329-d2wl6c9f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-38
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/d2wl6c9f
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k5_dot_product_norm1_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.17887
wandb:    train_loss 3.23725
wandb: training_time 1.48727
wandb:        val_f1 0.07628
wandb:      val_loss 3.22969
wandb: 
wandb: Synced lunar-sweep-38: https://wandb.ai/jah377/sffMHA_arxiv/runs/d2wl6c9f
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_113329-d2wl6c9f/logs
2022-06-16 11:52:14,740 - wandb.wandb_agent - INFO - Cleaning up finished run: d2wl6c9f
2022-06-16 11:52:17,813 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 11:52:17,813 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-16 11:52:17,820 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
2022-06-16 11:52:22,829 - wandb.wandb_agent - INFO - Running runs: ['ff0shj2j']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_115222-ff0shj2j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-39
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/ff0shj2j
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.27332
wandb:    train_loss 2.91312
wandb: training_time 0.86098
wandb:        val_f1 0.28743
wandb:      val_loss 2.8585
wandb: 
wandb: Synced fresh-sweep-39: https://wandb.ai/jah377/sffMHA_arxiv/runs/ff0shj2j
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_115222-ff0shj2j/logs
2022-06-16 12:01:34,999 - wandb.wandb_agent - INFO - Cleaning up finished run: ff0shj2j
2022-06-16 12:01:36,906 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:01:36,906 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.5
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 12:01:36,913 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.5 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 12:01:41,926 - wandb.wandb_agent - INFO - Running runs: ['kkdgcxjy']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_120141-kkdgcxjy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-40
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/kkdgcxjy
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.5, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.58668
wandb:    train_loss 1.39973
wandb: training_time 0.92459
wandb:        val_f1 0.59653
wandb:      val_loss 1.36645
wandb: 
wandb: Synced radiant-sweep-40: https://wandb.ai/jah377/sffMHA_arxiv/runs/kkdgcxjy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_120141-kkdgcxjy/logs
2022-06-16 12:12:16,385 - wandb.wandb_agent - INFO - Cleaning up finished run: kkdgcxjy
2022-06-16 12:12:17,240 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:12:17,240 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 12:12:17,248 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 12:12:22,262 - wandb.wandb_agent - INFO - Running runs: ['lzk5gk7s']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_121222-lzk5gk7s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-41
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/lzk5gk7s
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.60516
wandb:    train_loss 1.32999
wandb: training_time 0.96593
wandb:        val_f1 0.58978
wandb:      val_loss 1.40562
wandb: 
wandb: Synced lucky-sweep-41: https://wandb.ai/jah377/sffMHA_arxiv/runs/lzk5gk7s
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_121222-lzk5gk7s/logs
2022-06-16 12:23:01,783 - wandb.wandb_agent - INFO - Cleaning up finished run: lzk5gk7s
2022-06-16 12:23:02,349 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:23:02,349 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 12:23:02,356 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 12:23:07,370 - wandb.wandb_agent - INFO - Running runs: ['7mmvfwvk']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_122307-7mmvfwvk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-42
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/7mmvfwvk
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k3_dot_product_norm1_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65967
wandb:    train_loss 1.10403
wandb: training_time 0.91462
wandb:        val_f1 0.6125
wandb:      val_loss 1.2841
wandb: 
wandb: Synced vital-sweep-42: https://wandb.ai/jah377/sffMHA_arxiv/runs/7mmvfwvk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_122307-7mmvfwvk/logs
2022-06-16 12:38:15,378 - wandb.wandb_agent - INFO - Cleaning up finished run: 7mmvfwvk
2022-06-16 12:38:15,877 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:38:15,878 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 12:38:15,886 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 12:38:20,898 - wandb.wandb_agent - INFO - Running runs: ['0atz22pz']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_123820-0atz22pz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-43
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/0atz22pz
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.4109
wandb:    train_loss 2.18989
wandb: training_time 1.1406
wandb:        val_f1 0.41907
wandb:      val_loss 2.1145
wandb: 
wandb: Synced magic-sweep-43: https://wandb.ai/jah377/sffMHA_arxiv/runs/0atz22pz
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_123820-0atz22pz/logs
2022-06-16 12:50:01,572 - wandb.wandb_agent - INFO - Cleaning up finished run: 0atz22pz
2022-06-16 12:50:02,260 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 12:50:02,260 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 12:50:02,269 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 12:50:07,283 - wandb.wandb_agent - INFO - Running runs: ['crilksr8']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_125007-crilksr8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-44
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/crilksr8
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.58663
wandb:    train_loss 1.42333
wandb: training_time 0.95605
wandb:        val_f1 0.58707
wandb:      val_loss 1.41315
wandb: 
wandb: Synced deft-sweep-44: https://wandb.ai/jah377/sffMHA_arxiv/runs/crilksr8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_125007-crilksr8/logs
2022-06-16 13:00:45,330 - wandb.wandb_agent - INFO - Cleaning up finished run: crilksr8
2022-06-16 13:01:10,539 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:01:10,539 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.7
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 13:01:10,547 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.7 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 13:01:15,559 - wandb.wandb_agent - INFO - Running runs: ['7cpyd28b']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_130115-7cpyd28b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-45
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/7cpyd28b
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.7, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53824
wandb:    train_loss 1.68025
wandb: training_time 1.15822
wandb:        val_f1 0.54358
wandb:      val_loss 1.62597
wandb: 
wandb: Synced rose-sweep-45: https://wandb.ai/jah377/sffMHA_arxiv/runs/7cpyd28b
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_130115-7cpyd28b/logs
2022-06-16 13:13:29,334 - wandb.wandb_agent - INFO - Cleaning up finished run: 7cpyd28b
2022-06-16 13:13:29,820 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:13:29,821 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 13:13:29,830 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 13:13:34,843 - wandb.wandb_agent - INFO - Running runs: ['b1xps94l']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_131334-b1xps94l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-46
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/b1xps94l
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.63545
wandb:    train_loss 1.22014
wandb: training_time 0.99183
wandb:        val_f1 0.61838
wandb:      val_loss 1.26483
wandb: 
wandb: Synced wild-sweep-46: https://wandb.ai/jah377/sffMHA_arxiv/runs/b1xps94l
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_131334-b1xps94l/logs
2022-06-16 13:24:35,046 - wandb.wandb_agent - INFO - Cleaning up finished run: b1xps94l
2022-06-16 13:24:35,577 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:24:35,577 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 3
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 13:24:35,585 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=3 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 13:24:40,599 - wandb.wandb_agent - INFO - Running runs: ['fhyyccwm']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_132440-fhyyccwm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-47
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/fhyyccwm
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 3, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k3_dot_product_norm1_heads3.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.30801
wandb:    train_loss 4.35749
wandb: training_time 1.07561
wandb:        val_f1 0.37813
wandb:      val_loss 4.08038
wandb: 
wandb: Synced light-sweep-47: https://wandb.ai/jah377/sffMHA_arxiv/runs/fhyyccwm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_132440-fhyyccwm/logs
2022-06-16 13:40:30,534 - wandb.wandb_agent - INFO - Cleaning up finished run: fhyyccwm
2022-06-16 13:40:31,216 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:40:31,216 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 13:40:31,223 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 13:40:36,236 - wandb.wandb_agent - INFO - Running runs: ['t04wg4yo']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_134036-t04wg4yo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-48
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/t04wg4yo
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65513
wandb:    train_loss 1.13313
wandb: training_time 1.10288
wandb:        val_f1 0.62529
wandb:      val_loss 1.25215
wandb: 
wandb: Synced chocolate-sweep-48: https://wandb.ai/jah377/sffMHA_arxiv/runs/t04wg4yo
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_134036-t04wg4yo/logs
2022-06-16 13:52:02,314 - wandb.wandb_agent - INFO - Cleaning up finished run: t04wg4yo
2022-06-16 13:52:02,807 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:52:02,808 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-07
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 13:52:02,814 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-07 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 13:52:07,828 - wandb.wandb_agent - INFO - Running runs: ['6k6dqb2o']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_135207-6k6dqb2o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-49
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/6k6dqb2o
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-07, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
$$$ EARLY STOPPING TRIGGERED $$$
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 11
wandb:      train_f1 0.01814
wandb:    train_loss 3.70157
wandb: training_time 1.01282
wandb:        val_f1 0.01738
wandb:      val_loss 3.72804
wandb: 
wandb: Synced trim-sweep-49: https://wandb.ai/jah377/sffMHA_arxiv/runs/6k6dqb2o
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_135207-6k6dqb2o/logs
2022-06-16 13:52:43,927 - wandb.wandb_agent - INFO - Cleaning up finished run: 6k6dqb2o
2022-06-16 13:52:44,689 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 13:52:44,689 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 13:52:44,696 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 13:52:49,706 - wandb.wandb_agent - INFO - Running runs: ['kaoukq6q']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_135248-kaoukq6q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-50
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/kaoukq6q
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.61154
wandb:    train_loss 1.2863
wandb: training_time 1.08352
wandb:        val_f1 0.58952
wandb:      val_loss 1.38064
wandb: 
wandb: Synced worthy-sweep-50: https://wandb.ai/jah377/sffMHA_arxiv/runs/kaoukq6q
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_135248-kaoukq6q/logs
2022-06-16 14:04:15,296 - wandb.wandb_agent - INFO - Cleaning up finished run: kaoukq6q
2022-06-16 14:04:16,709 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:04:16,710 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 14:04:16,717 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 14:04:21,731 - wandb.wandb_agent - INFO - Running runs: ['h0abumdx']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_140421-h0abumdx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-51
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/h0abumdx
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.56228
wandb:    train_loss 1.4751
wandb: training_time 0.76686
wandb:        val_f1 0.54713
wandb:      val_loss 1.52199
wandb: 
wandb: Synced ruby-sweep-51: https://wandb.ai/jah377/sffMHA_arxiv/runs/h0abumdx
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_140421-h0abumdx/logs
2022-06-16 14:13:29,330 - wandb.wandb_agent - INFO - Cleaning up finished run: h0abumdx
2022-06-16 14:13:29,912 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:13:29,912 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 14:13:29,920 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 14:13:34,935 - wandb.wandb_agent - INFO - Running runs: ['n8kg2qn3']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_141334-n8kg2qn3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-52
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/n8kg2qn3
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65881
wandb:    train_loss 1.13346
wandb: training_time 0.83338
wandb:        val_f1 0.62227
wandb:      val_loss 1.26118
wandb: 
wandb: Synced genial-sweep-52: https://wandb.ai/jah377/sffMHA_arxiv/runs/n8kg2qn3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_141334-n8kg2qn3/logs
2022-06-16 14:23:28,815 - wandb.wandb_agent - INFO - Cleaning up finished run: n8kg2qn3
2022-06-16 14:23:29,617 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:23:29,617 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 14:23:29,625 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 14:23:34,640 - wandb.wandb_agent - INFO - Running runs: ['oegbm9na']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_142334-oegbm9na
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-53
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/oegbm9na
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k4_dot_product_norm1_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.55754
wandb:    train_loss 1.56578
wandb: training_time 1.56076
wandb:        val_f1 0.56445
wandb:      val_loss 1.59285
wandb: 
wandb: Synced sweet-sweep-53: https://wandb.ai/jah377/sffMHA_arxiv/runs/oegbm9na
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_142334-oegbm9na/logs
2022-06-16 14:43:00,616 - wandb.wandb_agent - INFO - Cleaning up finished run: oegbm9na
2022-06-16 14:43:01,149 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:43:01,150 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 0
	INCEPTION_LAYERS: 3
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 14:43:01,157 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=0 --INCEPTION_LAYERS=3 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 14:43:06,172 - wandb.wandb_agent - INFO - Running runs: ['6e32bwv0']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_144306-6e32bwv0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-54
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/6e32bwv0
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 3, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.50972
wandb:    train_loss 1.726
wandb: training_time 0.88923
wandb:        val_f1 0.49522
wandb:      val_loss 1.76464
wandb: 
wandb: Synced dashing-sweep-54: https://wandb.ai/jah377/sffMHA_arxiv/runs/6e32bwv0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_144306-6e32bwv0/logs
2022-06-16 14:53:15,045 - wandb.wandb_agent - INFO - Cleaning up finished run: 6e32bwv0
2022-06-16 14:53:15,773 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 14:53:15,773 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 14:53:15,781 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 14:53:20,794 - wandb.wandb_agent - INFO - Running runs: ['p70k16nd']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_145320-p70k16nd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-55
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/p70k16nd
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.58401
wandb:    train_loss 1.42862
wandb: training_time 0.8276
wandb:        val_f1 0.56455
wandb:      val_loss 1.50672
wandb: 
wandb: Synced amber-sweep-55: https://wandb.ai/jah377/sffMHA_arxiv/runs/p70k16nd
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_145320-p70k16nd/logs
2022-06-16 15:02:42,454 - wandb.wandb_agent - INFO - Cleaning up finished run: p70k16nd
2022-06-16 15:02:43,141 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:02:43,142 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 15:02:43,148 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 15:02:48,163 - wandb.wandb_agent - INFO - Running runs: ['4dchk7j8']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_150248-4dchk7j8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-56
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/4dchk7j8
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53423
wandb:    train_loss 1.88342
wandb: training_time 0.99938
wandb:        val_f1 0.55455
wandb:      val_loss 1.80129
wandb: 
wandb: Synced leafy-sweep-56: https://wandb.ai/jah377/sffMHA_arxiv/runs/4dchk7j8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_150248-4dchk7j8/logs
2022-06-16 15:14:10,457 - wandb.wandb_agent - INFO - Cleaning up finished run: 4dchk7j8
2022-06-16 15:14:11,102 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:14:11,103 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 15:14:11,110 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 15:14:16,124 - wandb.wandb_agent - INFO - Running runs: ['zxdun8ho']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_151416-zxdun8ho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-57
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/zxdun8ho
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.56358
wandb:    train_loss 1.47129
wandb: training_time 0.70893
wandb:        val_f1 0.54408
wandb:      val_loss 1.52803
wandb: 
wandb: Synced mild-sweep-57: https://wandb.ai/jah377/sffMHA_arxiv/runs/zxdun8ho
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_151416-zxdun8ho/logs
2022-06-16 15:22:41,417 - wandb.wandb_agent - INFO - Cleaning up finished run: zxdun8ho
2022-06-16 15:22:41,961 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:22:41,962 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 15:22:41,969 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 15:22:46,982 - wandb.wandb_agent - INFO - Running runs: ['g0py3ulu']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_152246-g0py3ulu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-58
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/g0py3ulu
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.66965
wandb:    train_loss 1.07541
wandb: training_time 1.05137
wandb:        val_f1 0.62378
wandb:      val_loss 1.24546
wandb: 
wandb: Synced dainty-sweep-58: https://wandb.ai/jah377/sffMHA_arxiv/runs/g0py3ulu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_152246-g0py3ulu/logs
2022-06-16 15:34:29,573 - wandb.wandb_agent - INFO - Cleaning up finished run: g0py3ulu
2022-06-16 15:34:30,116 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:34:30,117 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 15:34:30,125 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 15:34:35,139 - wandb.wandb_agent - INFO - Running runs: ['hcpb3oed']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_153435-hcpb3oed
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-59
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/hcpb3oed
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53498
wandb:    train_loss 1.59378
wandb: training_time 0.84688
wandb:        val_f1 0.54972
wandb:      val_loss 1.52995
wandb: 
wandb: Synced frosty-sweep-59: https://wandb.ai/jah377/sffMHA_arxiv/runs/hcpb3oed
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_153435-hcpb3oed/logs
2022-06-16 15:44:41,766 - wandb.wandb_agent - INFO - Cleaning up finished run: hcpb3oed
2022-06-16 15:44:42,364 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:44:42,364 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 15:44:42,372 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 15:44:47,385 - wandb.wandb_agent - INFO - Running runs: ['wqvzrycz']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_154447-wqvzrycz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-60
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/wqvzrycz
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.73003
wandb:    train_loss 0.85929
wandb: training_time 0.81955
wandb:        val_f1 0.62472
wandb:      val_loss 1.25343
wandb: 
wandb: Synced lilac-sweep-60: https://wandb.ai/jah377/sffMHA_arxiv/runs/wqvzrycz
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_154447-wqvzrycz/logs
2022-06-16 15:54:36,203 - wandb.wandb_agent - INFO - Cleaning up finished run: wqvzrycz
2022-06-16 15:54:38,994 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 15:54:38,995 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 15:54:39,002 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=512 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 15:54:44,014 - wandb.wandb_agent - INFO - Running runs: ['tmikivxv']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_155443-tmikivxv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-61
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/tmikivxv
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.69258
wandb:    train_loss 0.98045
wandb: training_time 0.77446
wandb:        val_f1 0.62539
wandb:      val_loss 1.24571
wandb: 
wandb: Synced swept-sweep-61: https://wandb.ai/jah377/sffMHA_arxiv/runs/tmikivxv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_155443-tmikivxv/logs
2022-06-16 16:03:54,385 - wandb.wandb_agent - INFO - Cleaning up finished run: tmikivxv
2022-06-16 16:03:54,973 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:03:54,974 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 16:03:54,981 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 16:03:59,994 - wandb.wandb_agent - INFO - Running runs: ['dqjpusbj']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_160359-dqjpusbj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-62
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/dqjpusbj
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.62729
wandb:    train_loss 1.22625
wandb: training_time 0.92143
wandb:        val_f1 0.60005
wandb:      val_loss 1.31958
wandb: 
wandb: Synced still-sweep-62: https://wandb.ai/jah377/sffMHA_arxiv/runs/dqjpusbj
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_160359-dqjpusbj/logs
2022-06-16 16:14:02,192 - wandb.wandb_agent - INFO - Cleaning up finished run: dqjpusbj
2022-06-16 16:14:02,882 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:14:02,882 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-07
2022-06-16 16:14:02,890 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-07
2022-06-16 16:14:07,902 - wandb.wandb_agent - INFO - Running runs: ['wqdisldl']
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_161409-wqdisldl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-63
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/wqdisldl
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-07, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.56031
wandb:    train_loss 1.49681
wandb: training_time 1.18018
wandb:        val_f1 0.57106
wandb:      val_loss 1.44432
wandb: 
wandb: Synced solar-sweep-63: https://wandb.ai/jah377/sffMHA_arxiv/runs/wqdisldl
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_161409-wqdisldl/logs
2022-06-16 16:26:08,226 - wandb.wandb_agent - INFO - Cleaning up finished run: wqdisldl
2022-06-16 16:26:09,872 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:26:09,872 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 16:26:09,879 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 16:26:14,893 - wandb.wandb_agent - INFO - Running runs: ['5yfz5azp']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_162614-5yfz5azp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-64
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/5yfz5azp
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.62704
wandb:    train_loss 1.21063
wandb: training_time 0.8444
wandb:        val_f1 0.56549
wandb:      val_loss 1.4628
wandb: 
wandb: Synced zesty-sweep-64: https://wandb.ai/jah377/sffMHA_arxiv/runs/5yfz5azp
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_162614-5yfz5azp/logs
2022-06-16 16:35:57,800 - wandb.wandb_agent - INFO - Cleaning up finished run: 5yfz5azp
2022-06-16 16:35:58,417 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:35:58,417 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 16:35:58,425 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 16:36:03,438 - wandb.wandb_agent - INFO - Running runs: ['ti8jam6p']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_163603-ti8jam6p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-65
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/ti8jam6p
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.6258
wandb:    train_loss 1.25449
wandb: training_time 0.83797
wandb:        val_f1 0.60311
wandb:      val_loss 1.33931
wandb: 
wandb: Synced logical-sweep-65: https://wandb.ai/jah377/sffMHA_arxiv/runs/ti8jam6p
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_163603-ti8jam6p/logs
2022-06-16 16:45:47,239 - wandb.wandb_agent - INFO - Cleaning up finished run: ti8jam6p
2022-06-16 16:45:48,352 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:45:48,353 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 16:45:48,360 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 16:45:53,375 - wandb.wandb_agent - INFO - Running runs: ['6r1rad40']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_164553-6r1rad40
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-66
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/6r1rad40
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.70008
wandb:    train_loss 0.97672
wandb: training_time 0.90005
wandb:        val_f1 0.63009
wandb:      val_loss 1.21718
wandb: 
wandb: Synced young-sweep-66: https://wandb.ai/jah377/sffMHA_arxiv/runs/6r1rad40
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_164553-6r1rad40/logs
2022-06-16 16:56:28,716 - wandb.wandb_agent - INFO - Cleaning up finished run: 6r1rad40
2022-06-16 16:56:29,257 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 16:56:29,257 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 16:56:29,265 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 16:56:34,279 - wandb.wandb_agent - INFO - Running runs: ['e3dj450k']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_165634-e3dj450k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-67
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/e3dj450k
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.80478
wandb:    train_loss 0.64831
wandb: training_time 0.8695
wandb:        val_f1 0.6271
wandb:      val_loss 1.26889
wandb: 
wandb: Synced dauntless-sweep-67: https://wandb.ai/jah377/sffMHA_arxiv/runs/e3dj450k
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_165634-e3dj450k/logs
2022-06-16 17:06:41,508 - wandb.wandb_agent - INFO - Cleaning up finished run: e3dj450k
2022-06-16 17:06:42,099 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 17:06:42,099 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 3
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-16 17:06:42,106 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=3 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
2022-06-16 17:06:47,121 - wandb.wandb_agent - INFO - Running runs: ['swdkl6we']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_170647-swdkl6we
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-68
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/swdkl6we
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 3, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k1_dot_product_norm1_heads3.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.574
wandb:    train_loss 1.61666
wandb: training_time 0.85139
wandb:        val_f1 0.58076
wandb:      val_loss 1.58692
wandb: 
wandb: Synced peachy-sweep-68: https://wandb.ai/jah377/sffMHA_arxiv/runs/swdkl6we
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_170647-swdkl6we/logs
2022-06-16 17:20:59,233 - wandb.wandb_agent - INFO - Cleaning up finished run: swdkl6we
2022-06-16 17:20:59,788 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 17:20:59,788 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 4
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 17:20:59,796 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=4 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 17:21:04,810 - wandb.wandb_agent - INFO - Running runs: ['o33si7i8']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_172104-o33si7i8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-69
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/o33si7i8
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.55034
wandb:    train_loss 1.55492
wandb: training_time 1.27912
wandb:        val_f1 0.56374
wandb:      val_loss 1.49454
wandb: 
wandb: Synced brisk-sweep-69: https://wandb.ai/jah377/sffMHA_arxiv/runs/o33si7i8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_172104-o33si7i8/logs
2022-06-16 17:33:49,918 - wandb.wandb_agent - INFO - Cleaning up finished run: o33si7i8
2022-06-16 17:33:50,449 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 17:33:50,450 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 5
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 17:33:50,459 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=5 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 17:33:55,472 - wandb.wandb_agent - INFO - Running runs: ['rq08e1pf']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_173355-rq08e1pf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-70
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/rq08e1pf
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 5, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k5_dot_product_norm1_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53888
wandb:    train_loss 1.60256
wandb: training_time 1.53263
wandb:        val_f1 0.54676
wandb:      val_loss 1.57396
wandb: 
wandb: Synced confused-sweep-70: https://wandb.ai/jah377/sffMHA_arxiv/runs/rq08e1pf
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_173355-rq08e1pf/logs
2022-06-16 17:53:28,032 - wandb.wandb_agent - INFO - Cleaning up finished run: rq08e1pf
2022-06-16 17:53:28,558 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 17:53:28,558 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 17:53:28,566 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 17:53:33,578 - wandb.wandb_agent - INFO - Running runs: ['jkrnz6jj']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_175333-jkrnz6jj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-71
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/jkrnz6jj
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.003 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.7149
wandb:    train_loss 0.92672
wandb: training_time 0.90916
wandb:        val_f1 0.62076
wandb:      val_loss 1.24947
wandb: 
wandb: Synced elated-sweep-71: https://wandb.ai/jah377/sffMHA_arxiv/runs/jkrnz6jj
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_175333-jkrnz6jj/logs
2022-06-16 18:04:04,239 - wandb.wandb_agent - INFO - Cleaning up finished run: jkrnz6jj
2022-06-16 18:04:05,952 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:04:05,952 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.6
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 18:04:05,959 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.6 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 18:04:10,970 - wandb.wandb_agent - INFO - Running runs: ['r5cmpwor']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_180410-r5cmpwor
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-72
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/r5cmpwor
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.6, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.46922
wandb:    train_loss 1.91708
wandb: training_time 1.32387
wandb:        val_f1 0.49505
wandb:      val_loss 1.81556
wandb: 
wandb: Synced soft-sweep-72: https://wandb.ai/jah377/sffMHA_arxiv/runs/r5cmpwor
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_180410-r5cmpwor/logs
2022-06-16 18:17:05,618 - wandb.wandb_agent - INFO - Cleaning up finished run: r5cmpwor
2022-06-16 18:17:06,374 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:17:06,374 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 18:17:06,382 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 18:17:11,394 - wandb.wandb_agent - INFO - Running runs: ['uicndbgf']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_181710-uicndbgf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-73
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/uicndbgf
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65345
wandb:    train_loss 1.15159
wandb: training_time 0.94436
wandb:        val_f1 0.60851
wandb:      val_loss 1.3061
wandb: 
wandb: Synced light-sweep-73: https://wandb.ai/jah377/sffMHA_arxiv/runs/uicndbgf
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_181710-uicndbgf/logs
2022-06-16 18:28:09,822 - wandb.wandb_agent - INFO - Cleaning up finished run: uicndbgf
2022-06-16 18:28:10,641 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:28:10,641 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 18:28:10,650 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 18:28:15,662 - wandb.wandb_agent - INFO - Running runs: ['pjyyu60d']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_182815-pjyyu60d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-74
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/pjyyu60d
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.52112
wandb:    train_loss 1.64206
wandb: training_time 0.87558
wandb:        val_f1 0.54173
wandb:      val_loss 1.57274
wandb: 
wandb: Synced lively-sweep-74: https://wandb.ai/jah377/sffMHA_arxiv/runs/pjyyu60d
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_182815-pjyyu60d/logs
2022-06-16 18:38:21,891 - wandb.wandb_agent - INFO - Cleaning up finished run: pjyyu60d
2022-06-16 18:38:22,564 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:38:22,565 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.7
	HOPS: 4
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 18:38:22,572 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.7 --HOPS=4 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 18:38:27,585 - wandb.wandb_agent - INFO - Running runs: ['yh4yrds6']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_183827-yh4yrds6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-75
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/yh4yrds6
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.7, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.54571
wandb:    train_loss 1.56688
wandb: training_time 0.85818
wandb:        val_f1 0.54807
wandb:      val_loss 1.55112
wandb: 
wandb: Synced icy-sweep-75: https://wandb.ai/jah377/sffMHA_arxiv/runs/yh4yrds6
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_183827-yh4yrds6/logs
2022-06-16 18:48:35,734 - wandb.wandb_agent - INFO - Cleaning up finished run: yh4yrds6
2022-06-16 18:48:36,354 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 18:48:36,355 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 18:48:36,361 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 18:48:41,375 - wandb.wandb_agent - INFO - Running runs: ['5h18q6oy']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_184841-5h18q6oy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-76
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/5h18q6oy
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64378
wandb:    train_loss 1.17998
wandb: training_time 1.31645
wandb:        val_f1 0.61505
wandb:      val_loss 1.27507
wandb: 
wandb: Synced autumn-sweep-76: https://wandb.ai/jah377/sffMHA_arxiv/runs/5h18q6oy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_184841-5h18q6oy/logs
2022-06-16 19:01:44,568 - wandb.wandb_agent - INFO - Cleaning up finished run: 5h18q6oy
2022-06-16 19:01:45,212 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:01:45,212 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 19:01:45,219 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 19:01:50,233 - wandb.wandb_agent - INFO - Running runs: ['nbiqnrig']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_190150-nbiqnrig
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-77
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/nbiqnrig
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53953
wandb:    train_loss 1.63458
wandb: training_time 1.16409
wandb:        val_f1 0.55485
wandb:      val_loss 1.56667
wandb: 
wandb: Synced pious-sweep-77: https://wandb.ai/jah377/sffMHA_arxiv/runs/nbiqnrig
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_190150-nbiqnrig/logs
2022-06-16 19:13:43,293 - wandb.wandb_agent - INFO - Cleaning up finished run: nbiqnrig
2022-06-16 19:13:44,128 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:13:44,128 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 4
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 19:13:44,137 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=4 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 19:13:49,150 - wandb.wandb_agent - INFO - Running runs: ['9yeahn7k']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_191349-9yeahn7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-78
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/9yeahn7k
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.55544
wandb:    train_loss 1.54175
wandb: training_time 0.98214
wandb:        val_f1 0.57089
wandb:      val_loss 1.48626
wandb: 
wandb: Synced sweet-sweep-78: https://wandb.ai/jah377/sffMHA_arxiv/runs/9yeahn7k
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_191349-9yeahn7k/logs
2022-06-16 19:25:06,418 - wandb.wandb_agent - INFO - Cleaning up finished run: 9yeahn7k
2022-06-16 19:25:07,260 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:25:07,261 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 19:25:07,270 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 19:25:12,283 - wandb.wandb_agent - INFO - Running runs: ['9uc2mavb']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_192512-9uc2mavb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-79
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/9uc2mavb
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65166
wandb:    train_loss 1.17421
wandb: training_time 0.9647
wandb:        val_f1 0.6118
wandb:      val_loss 1.30847
wandb: 
wandb: Synced azure-sweep-79: https://wandb.ai/jah377/sffMHA_arxiv/runs/9uc2mavb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_192512-9uc2mavb/logs
2022-06-16 19:35:12,465 - wandb.wandb_agent - INFO - Cleaning up finished run: 9uc2mavb
2022-06-16 19:35:13,116 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:35:13,116 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 19:35:13,124 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
2022-06-16 19:35:18,137 - wandb.wandb_agent - INFO - Running runs: ['4hnctmvx']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_193518-4hnctmvx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-80
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/4hnctmvx
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.53229
wandb:    train_loss 1.66236
wandb: training_time 1.19983
wandb:        val_f1 0.55294
wandb:      val_loss 1.58446
wandb: 
wandb: Synced amber-sweep-80: https://wandb.ai/jah377/sffMHA_arxiv/runs/4hnctmvx
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_193518-4hnctmvx/logs
2022-06-16 19:47:16,688 - wandb.wandb_agent - INFO - Cleaning up finished run: 4hnctmvx
2022-06-16 19:47:17,228 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:47:17,228 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 19:47:17,236 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 19:47:22,249 - wandb.wandb_agent - INFO - Running runs: ['v79k3mrl']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_194722-v79k3mrl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-81
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/v79k3mrl
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.64272
wandb:    train_loss 1.18076
wandb: training_time 1.05775
wandb:        val_f1 0.61717
wandb:      val_loss 1.27093
wandb: 
wandb: Synced wandering-sweep-81: https://wandb.ai/jah377/sffMHA_arxiv/runs/v79k3mrl
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_194722-v79k3mrl/logs
2022-06-16 19:58:45,530 - wandb.wandb_agent - INFO - Cleaning up finished run: v79k3mrl
2022-06-16 19:58:46,093 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 19:58:46,094 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 3
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 0
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 256
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 19:58:46,101 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=3 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=0 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=256 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 19:58:51,115 - wandb.wandb_agent - INFO - Running runs: ['qczd46d8']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_195851-qczd46d8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-82
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/qczd46d8
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 256, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 3, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.48292
wandb:    train_loss 1.79698
wandb: training_time 0.71824
wandb:        val_f1 0.49015
wandb:      val_loss 1.79034
wandb: 
wandb: Synced polar-sweep-82: https://wandb.ai/jah377/sffMHA_arxiv/runs/qczd46d8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_195851-qczd46d8/logs
2022-06-16 20:07:23,025 - wandb.wandb_agent - INFO - Cleaning up finished run: qczd46d8
2022-06-16 20:07:23,794 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:07:23,794 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 20:07:23,801 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 20:07:28,814 - wandb.wandb_agent - INFO - Running runs: ['1st0lfpo']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_200728-1st0lfpo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-83
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/1st0lfpo
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.55357
wandb:    train_loss 1.56713
wandb: training_time 1.20868
wandb:        val_f1 0.56552
wandb:      val_loss 1.52916
wandb: 
wandb: Synced fearless-sweep-83: https://wandb.ai/jah377/sffMHA_arxiv/runs/1st0lfpo
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_200728-1st0lfpo/logs
2022-06-16 20:19:29,994 - wandb.wandb_agent - INFO - Cleaning up finished run: 1st0lfpo
2022-06-16 20:19:30,481 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:19:30,481 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 4
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.01
2022-06-16 20:19:30,490 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=4 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.01
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 20:19:35,502 - wandb.wandb_agent - INFO - Running runs: ['l9jts8cr']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_201935-l9jts8cr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-84
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/l9jts8cr
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.01, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k4_dot_product_norm0_heads5.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.58394
wandb:    train_loss 1.44934
wandb: training_time 1.22305
wandb:        val_f1 0.57106
wandb:      val_loss 1.47251
wandb: 
wandb: Synced denim-sweep-84: https://wandb.ai/jah377/sffMHA_arxiv/runs/l9jts8cr
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_201935-l9jts8cr/logs
2022-06-16 20:37:02,095 - wandb.wandb_agent - INFO - Cleaning up finished run: l9jts8cr
2022-06-16 20:37:10,613 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:37:10,613 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 20:37:10,620 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 20:37:15,630 - wandb.wandb_agent - INFO - Running runs: ['mnk2emza']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_203715-mnk2emza
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-85
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/mnk2emza
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.63938
wandb:    train_loss 1.18349
wandb: training_time 0.94728
wandb:        val_f1 0.61851
wandb:      val_loss 1.2635
wandb: 
wandb: Synced woven-sweep-85: https://wandb.ai/jah377/sffMHA_arxiv/runs/mnk2emza
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_203715-mnk2emza/logs
2022-06-16 20:47:36,350 - wandb.wandb_agent - INFO - Cleaning up finished run: mnk2emza
2022-06-16 20:47:36,975 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:47:36,976 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 20:47:36,985 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 20:47:41,998 - wandb.wandb_agent - INFO - Running runs: ['6p7d9q29']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_204742-6p7d9q29
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-86
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/6p7d9q29
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.70931
wandb:    train_loss 0.92262
wandb: training_time 0.80638
wandb:        val_f1 0.6318
wandb:      val_loss 1.23003
wandb: 
wandb: Synced peach-sweep-86: https://wandb.ai/jah377/sffMHA_arxiv/runs/6p7d9q29
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_204742-6p7d9q29/logs
2022-06-16 20:57:16,517 - wandb.wandb_agent - INFO - Cleaning up finished run: 6p7d9q29
2022-06-16 20:57:17,051 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 20:57:17,052 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-05
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 20:57:17,060 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-05 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 20:57:22,070 - wandb.wandb_agent - INFO - Running runs: ['4fynwptd']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_205722-4fynwptd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-87
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/4fynwptd
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 1e-05, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.62846
wandb:    train_loss 1.29524
wandb: training_time 1.07131
wandb:        val_f1 0.60358
wandb:      val_loss 1.36601
wandb: 
wandb: Synced ruby-sweep-87: https://wandb.ai/jah377/sffMHA_arxiv/runs/4fynwptd
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_205722-4fynwptd/logs
2022-06-16 21:08:55,268 - wandb.wandb_agent - INFO - Cleaning up finished run: 4fynwptd
2022-06-16 21:08:55,791 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:08:55,792 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 2048
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 2
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 1e-06
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.001
2022-06-16 21:08:55,799 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=2048 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=2 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=1e-06 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 21:09:00,810 - wandb.wandb_agent - INFO - Running runs: ['8jwrj0a7']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_210900-8jwrj0a7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-88
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/8jwrj0a7
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 2048, 'LEARNING_RATE': 1e-06, 'WEIGHT_DECAY': 0.001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.52021
wandb:    train_loss 1.70109
wandb: training_time 1.28214
wandb:        val_f1 0.53193
wandb:      val_loss 1.64112
wandb: 
wandb: Synced jumping-sweep-88: https://wandb.ai/jah377/sffMHA_arxiv/runs/8jwrj0a7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_210900-8jwrj0a7/logs
2022-06-16 21:21:44,646 - wandb.wandb_agent - INFO - Cleaning up finished run: 8jwrj0a7
2022-06-16 21:21:45,337 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:21:45,337 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 0
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 21:21:45,344 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=0 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 21:21:50,354 - wandb.wandb_agent - INFO - Running runs: ['ujwwpd1x']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_212150-ujwwpd1x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-89
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/ujwwpd1x
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 0, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.61285
wandb:    train_loss 1.29786
wandb: training_time 0.83007
wandb:        val_f1 0.56542
wandb:      val_loss 1.4836
wandb: 
wandb: Synced young-sweep-89: https://wandb.ai/jah377/sffMHA_arxiv/runs/ujwwpd1x
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_212150-ujwwpd1x/logs
2022-06-16 21:31:28,393 - wandb.wandb_agent - INFO - Cleaning up finished run: ujwwpd1x
2022-06-16 21:31:29,312 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:31:29,313 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.4
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-06
2022-06-16 21:31:29,322 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.4 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-06
Using backend: pytorch
2022-06-16 21:31:34,336 - wandb.wandb_agent - INFO - Running runs: ['biqtoxhn']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_213134-biqtoxhn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-90
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/biqtoxhn
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 1e-06, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0.4, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.67216
wandb:    train_loss 1.07856
wandb: training_time 0.86239
wandb:        val_f1 0.62549
wandb:      val_loss 1.25202
wandb: 
wandb: Synced noble-sweep-90: https://wandb.ai/jah377/sffMHA_arxiv/runs/biqtoxhn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_213134-biqtoxhn/logs
2022-06-16 21:41:28,217 - wandb.wandb_agent - INFO - Cleaning up finished run: biqtoxhn
2022-06-16 21:41:31,450 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:41:31,450 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 2
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 21:41:31,458 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=2 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 21:41:36,472 - wandb.wandb_agent - INFO - Running runs: ['gjv32kz9']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_214136-gjv32kz9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-91
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/gjv32kz9
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 2, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.75888
wandb:    train_loss 0.75751
wandb: training_time 0.87604
wandb:        val_f1 0.6312
wandb:      val_loss 1.24211
wandb: 
wandb: Synced brisk-sweep-91: https://wandb.ai/jah377/sffMHA_arxiv/runs/gjv32kz9
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_214136-gjv32kz9/logs
2022-06-16 21:51:40,713 - wandb.wandb_agent - INFO - Cleaning up finished run: gjv32kz9
2022-06-16 21:51:41,212 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 21:51:41,212 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 21:51:41,219 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 21:51:46,230 - wandb.wandb_agent - INFO - Running runs: ['xq3uqjoc']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_215146-xq3uqjoc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-92
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/xq3uqjoc
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.63002
wandb:    train_loss 1.22701
wandb: training_time 0.94565
wandb:        val_f1 0.59193
wandb:      val_loss 1.35571
wandb: 
wandb: Synced prime-sweep-92: https://wandb.ai/jah377/sffMHA_arxiv/runs/xq3uqjoc
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_215146-xq3uqjoc/logs
2022-06-16 22:01:40,415 - wandb.wandb_agent - INFO - Cleaning up finished run: xq3uqjoc
2022-06-16 22:01:41,059 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:01:41,059 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 3
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.3
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.1
2022-06-16 22:01:41,068 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=3 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.3 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.1
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 22:01:46,078 - wandb.wandb_agent - INFO - Running runs: ['hdpt02zi']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_220146-hdpt02zi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-93
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/hdpt02zi
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 0.1, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.3, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 3, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.28643
wandb:    train_loss 2.7977
wandb: training_time 0.9775
wandb:        val_f1 0.30407
wandb:      val_loss 2.70571
wandb: 
wandb: Synced lunar-sweep-93: https://wandb.ai/jah377/sffMHA_arxiv/runs/hdpt02zi
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_220146-hdpt02zi/logs
2022-06-16 22:12:23,720 - wandb.wandb_agent - INFO - Cleaning up finished run: hdpt02zi
2022-06-16 22:12:24,230 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:12:24,231 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 22:12:24,239 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 22:12:29,252 - wandb.wandb_agent - INFO - Running runs: ['f1yw5g8z']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_221229-f1yw5g8z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-94
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/f1yw5g8z
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.66383
wandb:    train_loss 1.07391
wandb: training_time 1.34547
wandb:        val_f1 0.61807
wandb:      val_loss 1.26903
wandb: 
wandb: Synced exalted-sweep-94: https://wandb.ai/jah377/sffMHA_arxiv/runs/f1yw5g8z
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_221229-f1yw5g8z/logs
2022-06-16 22:25:48,490 - wandb.wandb_agent - INFO - Cleaning up finished run: f1yw5g8z
2022-06-16 22:25:49,663 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:25:49,664 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 128
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.2
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 512
	LEARNING_RATE: 0.01
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 22:25:49,672 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=128 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.2 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=512 --LEARNING_RATE=0.01 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
2022-06-16 22:25:54,686 - wandb.wandb_agent - INFO - Running runs: ['8mvtbzy2']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_222554-8mvtbzy2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-95
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/8mvtbzy2
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.01, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 512, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 128, 'FEATURE_DROPOUT': 0.2, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.65998
wandb:    train_loss 1.09345
wandb: training_time 0.75111
wandb:        val_f1 0.61995
wandb:      val_loss 1.24922
wandb: 
wandb: Synced hopeful-sweep-95: https://wandb.ai/jah377/sffMHA_arxiv/runs/8mvtbzy2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_222554-8mvtbzy2/logs
2022-06-16 22:34:31,389 - wandb.wandb_agent - INFO - Cleaning up finished run: 8mvtbzy2
2022-06-16 22:34:32,004 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:34:32,004 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0
	HOPS: 4
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 22:34:32,012 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0 --HOPS=4 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 22:34:37,026 - wandb.wandb_agent - INFO - Running runs: ['fz1lrwuy']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_223436-fz1lrwuy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-96
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/fz1lrwuy
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 4, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.75601
wandb:    train_loss 0.8309
wandb: training_time 1.16588
wandb:        val_f1 0.62801
wandb:      val_loss 1.27337
wandb: 
wandb: Synced glad-sweep-96: https://wandb.ai/jah377/sffMHA_arxiv/runs/fz1lrwuy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_223436-fz1lrwuy/logs
2022-06-16 22:46:49,415 - wandb.wandb_agent - INFO - Cleaning up finished run: fz1lrwuy
2022-06-16 22:46:50,011 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:46:50,012 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 1024
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.3
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 22:46:50,018 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=1024 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.3 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 22:46:55,030 - wandb.wandb_agent - INFO - Running runs: ['6rrec1m5']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_224655-6rrec1m5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-97
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/6rrec1m5
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 1024, 'FEATURE_DROPOUT': 0.3, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.67908
wandb:    train_loss 1.03092
wandb: training_time 0.90297
wandb:        val_f1 0.60908
wandb:      val_loss 1.29366
wandb: 
wandb: Synced hopeful-sweep-97: https://wandb.ai/jah377/sffMHA_arxiv/runs/6rrec1m5
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_224655-6rrec1m5/logs
2022-06-16 22:56:48,254 - wandb.wandb_agent - INFO - Cleaning up finished run: 6rrec1m5
2022-06-16 22:56:48,780 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 22:56:48,781 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 16384
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.1
	HOPS: 1
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.1
	LR_PATIENCE: 5
	NODE_DROPOUT: 0
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 22:56:48,788 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=16384 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.1 --HOPS=1 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.1 --LR_PATIENCE=5 --NODE_DROPOUT=0 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 22:56:53,803 - wandb.wandb_agent - INFO - Running runs: ['0xr9kdg0']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_225653-0xr9kdg0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-98
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/0xr9kdg0
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 1, 'BATCH_SIZE': 16384, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.1, 'NODE_DROPOUT': 0, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.5991
wandb:    train_loss 1.35336
wandb: training_time 0.76873
wandb:        val_f1 0.59599
wandb:      val_loss 1.37545
wandb: 
wandb: Synced peachy-sweep-98: https://wandb.ai/jah377/sffMHA_arxiv/runs/0xr9kdg0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_225653-0xr9kdg0/logs
2022-06-16 23:06:05,808 - wandb.wandb_agent - INFO - Cleaning up finished run: 0xr9kdg0
2022-06-16 23:06:06,299 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 23:06:06,300 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 4
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 8192
	CLASSIFICATION_LAYERS: 2
	CLASSIFICATION_UNITS: 256
	DATASET: arxiv
	DPA_NORMALIZATION: 0
	EPOCHS: 300
	FEATURE_DROPOUT: 0.4
	HOPS: 3
	INCEPTION_LAYERS: 2
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.0001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.1
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 1e-05
2022-06-16 23:06:06,307 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=4 --BATCH_NORMALIZATION=1 --BATCH_SIZE=8192 --CLASSIFICATION_LAYERS=2 --CLASSIFICATION_UNITS=256 --DATASET=arxiv --DPA_NORMALIZATION=0 --EPOCHS=300 --FEATURE_DROPOUT=0.4 --HOPS=3 --INCEPTION_LAYERS=2 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.0001 --LR_PATIENCE=5 --NODE_DROPOUT=0.1 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=1e-05
Using backend: pytorch
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
2022-06-16 23:06:11,318 - wandb.wandb_agent - INFO - Running runs: ['hel09ti4']
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_230611-hel09ti4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-99
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/hel09ti4
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 8192, 'LEARNING_RATE': 0.0001, 'WEIGHT_DECAY': 1e-05, 'INCEPTION_LAYERS': 2, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 2, 'CLASSIFICATION_UNITS': 256, 'FEATURE_DROPOUT': 0.4, 'NODE_DROPOUT': 0.1, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 4, 'DPA_NORMALIZATION': 0, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}

TRANSFORMED FILE: data/arxiv_k3_dot_product_norm0_heads4.pth

wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.62832
wandb:    train_loss 1.25452
wandb: training_time 1.24711
wandb:        val_f1 0.60166
wandb:      val_loss 1.34523
wandb: 
wandb: Synced eager-sweep-99: https://wandb.ai/jah377/sffMHA_arxiv/runs/hel09ti4
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_230611-hel09ti4/logs
2022-06-16 23:23:32,258 - wandb.wandb_agent - INFO - Cleaning up finished run: hel09ti4
2022-06-16 23:23:33,003 - wandb.wandb_agent - INFO - Agent received command: run
2022-06-16 23:23:33,004 - wandb.wandb_agent - INFO - Agent starting run with config:
	ATTN_HEADS: 5
	BATCH_NORMALIZATION: 1
	BATCH_SIZE: 4096
	CLASSIFICATION_LAYERS: 3
	CLASSIFICATION_UNITS: 512
	DATASET: arxiv
	DPA_NORMALIZATION: 1
	EPOCHS: 300
	FEATURE_DROPOUT: 0.5
	HOPS: 3
	INCEPTION_LAYERS: 1
	INCEPTION_UNITS: 1024
	LEARNING_RATE: 0.001
	LR_PATIENCE: 5
	NODE_DROPOUT: 0.2
	SEED: 42
	TERMINATION_PATIENCE: 10
	TRANSFORMATION: dot_product
	WEIGHT_DECAY: 0.0001
2022-06-16 23:23:33,010 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python hps_SIGNff_DPA.py --ATTN_HEADS=5 --BATCH_NORMALIZATION=1 --BATCH_SIZE=4096 --CLASSIFICATION_LAYERS=3 --CLASSIFICATION_UNITS=512 --DATASET=arxiv --DPA_NORMALIZATION=1 --EPOCHS=300 --FEATURE_DROPOUT=0.5 --HOPS=3 --INCEPTION_LAYERS=1 --INCEPTION_UNITS=1024 --LEARNING_RATE=0.001 --LR_PATIENCE=5 --NODE_DROPOUT=0.2 --SEED=42 --TERMINATION_PATIENCE=10 --TRANSFORMATION=dot_product --WEIGHT_DECAY=0.0001
Using backend: pytorch
2022-06-16 23:23:38,022 - wandb.wandb_agent - INFO - Running runs: ['pjpuj9x0']
wandb: Currently logged in as: jah377. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /scratch/sffMHA_arxiv/wandb/run-20220616_232338-pjpuj9x0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-100
wandb:  View project at https://wandb.ai/jah377/sffMHA_arxiv
wandb:  View sweep at https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb:  View run at https://wandb.ai/jah377/sffMHA_arxiv/runs/pjpuj9x0
{'DATASET': 'arxiv', 'SEED': 42, 'EPOCHS': 300, 'HOPS': 3, 'BATCH_SIZE': 4096, 'LEARNING_RATE': 0.001, 'WEIGHT_DECAY': 0.0001, 'INCEPTION_LAYERS': 1, 'INCEPTION_UNITS': 1024, 'CLASSIFICATION_LAYERS': 3, 'CLASSIFICATION_UNITS': 512, 'FEATURE_DROPOUT': 0.5, 'NODE_DROPOUT': 0.2, 'BATCH_NORMALIZATION': 1, 'ATTN_HEADS': 5, 'DPA_NORMALIZATION': 1, 'LR_PATIENCE': 5, 'TERMINATION_PATIENCE': 10, 'TRANSFORMATION': 'dot_product'}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:         epoch 300
wandb:      train_f1 0.61113
wandb:    train_loss 1.34312
wandb: training_time 1.00457
wandb:        val_f1 0.59133
wandb:      val_loss 1.40496
wandb: 
wandb: Synced astral-sweep-100: https://wandb.ai/jah377/sffMHA_arxiv/runs/pjpuj9x0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_232338-pjpuj9x0/logs
2022-06-16 23:34:22,894 - wandb.wandb_agent - INFO - Cleaning up finished run: pjpuj9x0
wandb: Terminating and syncing runs. Press ctrl-c to kill.
Create sweep with ID: 9mpsmk6h
Sweep URL: https://wandb.ai/jah377/sffMHA_arxiv/sweeps/9mpsmk6h
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: - 0.205 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: \ 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: | 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: / 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: - 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: \ 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: | 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: / 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: - 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb: \ 0.268 MB of 0.268 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced giddy-pond-99: https://wandb.ai/jah377/sffMHA_arxiv/runs/2o824h0n
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220616_034641-2o824h0n/logs
==================================================

++++++++++++++++++++++++++++++++++++++++++++++++++++
TOTAL RUNTIME = 19 hours 48 minutes
++++++++++++++++++++++++++++++++++++++++++++++++++++
