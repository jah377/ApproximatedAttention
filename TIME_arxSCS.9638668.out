Wed 22 Jun 2022 02:38:55 PM CEST
r30n7.lisa.surfsara.nl
uid=55639(jharris) gid=55199(jharris) groups=55199(jharris),46457(lisa_uva_gpu),50488(ssh_forwarding)
/home/jharris/Desktop/approx_attention
/home/jharris/Desktop/approx_attention/venvs/GPU_venv/bin/python

Check if packages installed correctly
Torch: 1.11.0+cu113
PyG: 2.0.4

==================================================
Using backend: pytorch
usage: sign_runs.py [-h] [--DATASET DATASET] [--ATTN_FILTER ATTN_FILTER]
                    [--EPOCHS EPOCHS] [--EVAL_EVERY EVAL_EVERY]
                    [--RUN_SEEDS RUN_SEEDS] [--HOPS HOPS]
                    [--BATCH_SIZE BATCH_SIZE] [--LEARNING_RATE LEARNING_RATE]
                    [--WEIGHT_DECAY WEIGHT_DECAY]
                    [--INCEPTION_LAYERS INCEPTION_LAYERS]
                    [--INCEPTION_UNITS INCEPTION_UNITS]
                    [--CLASSIFICATION_LAYERS CLASSIFICATION_LAYERS]
                    [--CLASSIFICATION_UNITS CLASSIFICATION_UNITS]
                    [--FEATURE_DROPOUT FEATURE_DROPOUT]
                    [--NODE_DROPOUT NODE_DROPOUT]
                    [--BATCH_NORMALIZATION BATCH_NORMALIZATION]
                    [--ATTN_HEADS ATTN_HEADS]
                    [--ATTN_NORMALIZATION ATTN_NORMALIZATION]
                    [--GAT_EPOCHS GAT_EPOCHS]
                    [--GAT_BATCH_SIZE GAT_BATCH_SIZE]
                    [--GAT_LEARNING_RATE GAT_LEARNING_RATE]
                    [--GAT_WEIGHT_DECAY GAT_WEIGHT_DECAY]
                    [--GAT_HIDDEN_UNITS GAT_HIDDEN_UNITS]
                    [--GAT_NODE_DROPOUT GAT_NODE_DROPOUT]
                    [--GAT_LAYERS GAT_LAYERS] [--GAT_HEADS_IN GAT_HEADS_IN]
                    [--GAT_HEADS_OUT GAT_HEADS_OUT]
                    [--GAT_NEIGHBORS GAT_NEIGHBORS]
                    [--GAT_LR_PATIENCE GAT_LR_PATIENCE]
sign_runs.py: error: argument --ATTN_HEADS: expected one argument
==================================================

++++++++++++++++++++++++++++++++++++++++++++++++++++
TOTAL RUNTIME = 0 hours 00 minutes
++++++++++++++++++++++++++++++++++++++++++++++++++++
