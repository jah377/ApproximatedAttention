#!/bin/bash
#SBATCH --job-name="TIME_pubmed"
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --time=24:00:00
#SBATCH --gpus=1
#SBATCH --partition=gpu_shared
#SBATCH --output=%x.%j.out
#SBATCH --mail-user=jon.harris@student.uva.nl
#SBATCH --mail-type=BEGIN,END,FAIL

date;hostname;id;pwd
START=`date +%s`                            # will record .job runtime

DATASET='pubmed'                            # dataset used
PY_FILE='sign_runs.py'                      # file to sweep
SCRATCH_FOLDER='TIME_pubmed'                # name of folder on TMPDIR
EXPERIMENT_FOLDER='RUNS'                    # folder containing files

# inputs for all SIGN-based models
EVAL_EVERY=10
RUN_SEEDS='0,4,8,42,64,128,256,512,1024,2048'
EPOCHS=200

ATTN_NORMALIZATION=1
BATCH_NORMALIZATION=1
BATCH_SIZE=256
CLASSIFICATION_LAYERS=3
CLASSIFICATION_UNITS=512
FEATURE_DROPOUT=0.3
HOPS=2
INCEPTION_LAYERS=2
INCEPTION_UNITS=512
LEARNING_RATE=1e-3
NODE_DROPOUT=0.3
WEIGHT_DECAY=1e-7


# copy files to scratch
mkdir -p $TMPDIR/$SCRATCH_FOLDER/data
cp -r data/pubmed/pubmed_sign_k0.pth $TMPDIR/$SCRATCH_FOLDER/data
# cp -r data/arxiv/arxiv_sign_k0.pth $TMPDIR/$SCRATCH_FOLDER/data
# cp -r data/products/products_sign_k0.pth $TMPDIR/$SCRATCH_FOLDER/data
cp -r venvs $TMPDIR/$SCRATCH_FOLDER
cp -r experiments/$EXPERIMENT_FOLDER/* $TMPDIR/$SCRATCH_FOLDER
cd $TMPDIR/$SCRATCH_FOLDER


# activate venv
module load 2021
module load CUDA/11.3.1
source $TMPDIR/$SCRATCH_FOLDER/venvs/GPU_venv/bin/activate
which python

echo ''
echo 'Check if packages installed correctly'
python -c "import torch; print(f'Torch: {torch.__version__}')"
python -c "import torch_geometric; print(f'PyG: {torch_geometric.__version__}')"
echo ''

echo '=================================================='
echo '================= SIGN ==========================='
echo '=================================================='

ATTN_FILTER='sign'

python $PY_FILE\
    --DATASET $DATASET \
    --ATTN_FILTER $ATTN_FILTER \
    --EPOCHS $EPOCHS \
    --EVAL_EVERY $EVAL_EVERY \
    --RUN_SEEDS $RUN_SEEDS  \
    --HOPS $HOPS \
    --BATCH_SIZE $BATCH_SIZE \
    --LEARNING_RATE $LEARNING_RATE \
    --WEIGHT_DECAY $WEIGHT_DECAY \
    --INCEPTION_LAYERS $INCEPTION_LAYERS \
    --INCEPTION_UNITS $INCEPTION_UNITS \
    --CLASSIFICATION_LAYERS $CLASSIFICATION_LAYERS \
    --CLASSIFICATION_UNITS $CLASSIFICATION_UNITS \
    --FEATURE_DROPOUT $FEATURE_DROPOUT \
    --NODE_DROPOUT $NODE_DROPOUT \
    --BATCH_NORMALIZATION $BATCH_NORMALIZATION

echo '=================================================='
echo ''

echo '=================================================='
echo '================= SIGN+GAT ======================='
echo '=================================================='

ATTN_FILTER='gat'
GAT_EPOCHS=300
GAT_BATCH_SIZE=1024
GAT_LEARNING_RATE=0.01
GAT_WEIGHT_DECAY=0.001
GAT_NODE_DROPOUT=0.6
GAT_HIDDEN_UNITS=8
GAT_LAYERS=2
GAT_HEADS_IN=8
GAT_HEADS_OUT=8
GAT_NEIGHBORS=150

python $PY_FILE\
    --DATASET $DATASET \
    --ATTN_FILTER $ATTN_FILTER \
    --EPOCHS $EPOCHS \
    --EVAL_EVERY $EVAL_EVERY \
    --RUN_SEEDS $RUN_SEEDS  \
    --HOPS $HOPS \
    --BATCH_SIZE $BATCH_SIZE \
    --LEARNING_RATE $LEARNING_RATE \
    --WEIGHT_DECAY $WEIGHT_DECAY \
    --INCEPTION_LAYERS $INCEPTION_LAYERS \
    --INCEPTION_UNITS $INCEPTION_UNITS \
    --CLASSIFICATION_LAYERS $CLASSIFICATION_LAYERS \
    --CLASSIFICATION_UNITS $CLASSIFICATION_UNITS \
    --FEATURE_DROPOUT $FEATURE_DROPOUT \
    --NODE_DROPOUT $NODE_DROPOUT \
    --BATCH_NORMALIZATION $BATCH_NORMALIZATION \
    --GAT_EPOCHS $GAT_EPOCHS \
    --GAT_BATCH_SIZE $GAT_BATCH_SIZE \
    --GAT_LEARNING_RATE $GAT_LEARNING_RATE \
    --GAT_WEIGHT_DECAY $GAT_WEIGHT_DECAY \
    --GAT_HIDDEN_UNITS $GAT_HIDDEN_UNITS \
    --GAT_NODE_DROPOUT $GAT_NODE_DROPOUT \
    --GAT_LAYERS $GAT_LAYERS \
    --GAT_HEADS_IN $GAT_HEADS_IN \
    --GAT_HEADS_OUT $GAT_HEADS_OUT \
    --GAT_NEIGHBORS $GAT_NEIGHBORS 

echo '=================================================='
echo ''



echo '=================================================='
echo '================= GAT ==========================='
echo '=================================================='

python gat_runs.py \
    --DATASET $DATASET \
    --EPOCHS $GAT_EPOCHS \
    --EVAL_EVERY $EVAL_EVERY \
    --RUN_SEEDS $RUN_SEEDS  \
    --BATCH_SIZE $GAT_BATCH_SIZE \
    --LEARNING_RATE $GAT_LEARNING_RATE \
    --WEIGHT_DECAY $GAT_WEIGHT_DECAY \
    --NODE_DROPOUT $NODE_DROPOUT \
    --HIDDEN_UNITS $GAT_HIDDEN_UNITS \
    --LAYERS $GAT_LAYERS \
    --HEADS_IN $GAT_HEADS_IN \
    --HEADS_OUT $GAT_HEADS_OUT \
    --NEIGHBORS $GAT_NEIGHBORS

echo '=================================================='
echo ''



echo '=================================================='
echo '================= SIGN+CS ========================'
echo '=================================================='

ATTN_FILTER='cosine'

python $PY_FILE \
    --DATASET $DATASET \
    --ATTN_FILTER $ATTN_FILTER \
    --EPOCHS $EPOCHS \
    --EVAL_EVERY $EVAL_EVERY \
    --RUN_SEEDS $RUN_SEEDS  \
    --HOPS $HOPS \
    --BATCH_SIZE $BATCH_SIZE \
    --LEARNING_RATE $LEARNING_RATE \
    --WEIGHT_DECAY $WEIGHT_DECAY \
    --INCEPTION_LAYERS $INCEPTION_LAYERS \
    --INCEPTION_UNITS $INCEPTION_UNITS \
    --CLASSIFICATION_LAYERS $CLASSIFICATION_LAYERS \
    --CLASSIFICATION_UNITS $CLASSIFICATION_UNITS \
    --FEATURE_DROPOUT $FEATURE_DROPOUT \
    --NODE_DROPOUT $NODE_DROPOUT \
    --BATCH_NORMALIZATION $BATCH_NORMALIZATION \
    --ATTN_NORMALIZATION $ATTN_NORMALIZATION 

echo '=================================================='
echo ''



echo '=================================================='
echo '================= SIGN+SHA ======================='
echo '=================================================='

ATTN_FILTER='dot_product'
ATTN_HEADS=1

python $PY_FILE\
    --DATASET $DATASET \
    --ATTN_FILTER $ATTN_FILTER \
    --EPOCHS $EPOCHS \
    --EVAL_EVERY $EVAL_EVERY \
    --RUN_SEEDS $RUN_SEEDS  \
    --HOPS $HOPS \
    --BATCH_SIZE $BATCH_SIZE \
    --LEARNING_RATE $LEARNING_RATE \
    --WEIGHT_DECAY $WEIGHT_DECAY \
    --INCEPTION_LAYERS $INCEPTION_LAYERS \
    --INCEPTION_UNITS $INCEPTION_UNITS \
    --CLASSIFICATION_LAYERS $CLASSIFICATION_LAYERS \
    --CLASSIFICATION_UNITS $CLASSIFICATION_UNITS \
    --FEATURE_DROPOUT $FEATURE_DROPOUT \
    --NODE_DROPOUT $NODE_DROPOUT \
    --BATCH_NORMALIZATION $BATCH_NORMALIZATION \
    --ATTN_NORMALIZATION $ATTN_NORMALIZATION \
    --ATTN_HEADS $ATTN_HEADS


echo '=================================================='
echo ''



echo '=================================================='
echo '================= SIGN+MHA ======================='
echo '=================================================='

ATTN_FILTER='dot_product'
ATTN_HEADS=5

python $PY_FILE\
    --DATASET $DATASET \
    --ATTN_FILTER $ATTN_FILTER \
    --EPOCHS $EPOCHS \
    --EVAL_EVERY $EVAL_EVERY \
    --RUN_SEEDS $RUN_SEEDS  \
    --HOPS $HOPS \
    --BATCH_SIZE $BATCH_SIZE \
    --LEARNING_RATE $LEARNING_RATE \
    --WEIGHT_DECAY $WEIGHT_DECAY \
    --INCEPTION_LAYERS $INCEPTION_LAYERS \
    --INCEPTION_UNITS $INCEPTION_UNITS \
    --CLASSIFICATION_LAYERS $CLASSIFICATION_LAYERS \
    --CLASSIFICATION_UNITS $CLASSIFICATION_UNITS \
    --FEATURE_DROPOUT $FEATURE_DROPOUT \
    --NODE_DROPOUT $NODE_DROPOUT \
    --BATCH_NORMALIZATION $BATCH_NORMALIZATION \
    --ATTN_NORMALIZATION $ATTN_NORMALIZATION \
    --ATTN_HEADS $ATTN_HEADS

echo '=================================================='
echo ''


echo '++++++++++++++++++++++++++++++++++++++++++++++++++++'
END=`date +%s`;

let DIFF=END-START
let HOURS=DIFF/3600
let MINUTES=(DIFF/60)%60

printf 'TOTAL RUNTIME = %d hours %02d minutes\n' $HOURS $MINUTES
echo '++++++++++++++++++++++++++++++++++++++++++++++++++++'
